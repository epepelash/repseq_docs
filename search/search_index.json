{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#mixcr-analyze","title":"MiXCR analyze","text":"<pre><code># Will be made executable with markdown-exec (although there should be other options)\n# Not ready yet :(\nimport os\nimport pandas as pd\nfrom IPython.display import Image, display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>REPSEQ_PATH = '/home/epepeliaeva/soft/repseq/'\n\nimport sys\n\nsys.path.append(REPSEQ_PATH)\nfrom repseq import io as repseqio\nfrom repseq import mixcr as mx\nfrom repseq import slurm\nfrom repseq import clonosets as cl\nfrom repseq import stats\nfrom repseq import clone_filter as clf\nfrom repseq import intersections\nfrom repseq import clustering\nfrom repseq import logo\nfrom repseq import vdjtools\n</code></pre> <p><pre><code>MIXCR = \"/projects/cdr3_software/bin/mixcr\"\n\nWORKING_DIR = \"/projects/cdr3_common/repseq_demo/\"\nMIXCR_DIR = os.path.join(WORKING_DIR, \"mixcr\")\n\nRAW_DATA_DIR = \"/projects/cdr3_ngs/2023/11_room555_MiSeq_13112023/\"\n\nSAMPLE_LIST_FILENAME = os.path.join(WORKING_DIR, \"sample_table.csv\")\nTABLE_REPORT_FILENAME = os.path.join(WORKING_DIR, \"table_report.csv\")\n\nos.makedirs(WORKING_DIR, exist_ok=True)\n</code></pre> <pre><code>WORKING_DIR = \"/projects/cdr3_common/repseq_demo/\"\nMIXCR_DIR = os.path.join(WORKING_DIR, \"mixcr\")\n\nRAW_DATA_DIR = \"/projects/cdr3_ngs/2023/11_room555_MiSeq_13112023/\"\n\nSAMPLE_LIST_FILENAME = os.path.join(WORKING_DIR, \"sample_table.csv\")\nTABLE_REPORT_FILENAME = os.path.join(WORKING_DIR, \"table_report.csv\")\n\nsample_df = repseqio.read_yaml_metadata(RAW_DATA_DIR)[[\"sample_id\", \"R1\", \"R2\"]].query('sample_id.str.contains(\"Rev05\")')\n\nmx.mixcr4_analyze_batch(sample_df=sample_df, \n                        output_folder = MIXCR_DIR, \n                        command_template=None,\n                        mixcr_path=MIXCR, \n                        memory=32, \n                        time_estimate=1.5)\n</code></pre></p> <pre><code>slurm.check_slurm_progress(os.path.join(MIXCR_DIR, \"mixcr_analyze_slurm_batch.log\"), loop=True)\n</code></pre> <pre><code>mx.show_report_images(MIXCR_DIR)\n</code></pre> <pre><code>proc_table = mx.get_processing_table(MIXCR_DIR)\nproc_table.to_csv(TABLE_REPORT_FILENAME, index=False)\nprint(f\"Report table saved to: {TABLE_REPORT_FILENAME}\")\nproc_table\n</code></pre>"},{"location":"filter/","title":"Filter","text":"<p>Clonoset filter. May be used to filter clonosets:     - by clone functionality     - randomly downsample them to particular number of reads of UMIs     - take top clonotypes by size (number of reads of UMIs) with or without random mixing</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the filter. Will be displayed in print</p> <code>'default_filter'</code> <code>functionality</code> <code>str</code> <p>Possible values: - \"a\" - any (default). No clones are filtered out - \"f\" - only functional. Those, not having stop codons and      frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid     sequence - \"n\" - only-nonfunctional - opposite to \"f\" - functional</p> <code>'a'</code> <code>downsample</code> <code>int</code> <p>the number of reads/UMIs to randomly downsample the clonoset to. default value 'None' - means not to apply downsampling</p> <code>None</code> <code>top</code> <code>int</code> <p>the number of top biggest by reads/UMIs clonotypes to take from the clonoset. default value 'None' - means not to apply top</p> <code>None</code> <code>by_umi</code> <code>bool</code> <p>default=False. Which column to take for clonotype count - reads or UMIs  (if UMI count column exists).</p> <code>False</code> <code>mix_tails</code> <code>bool</code> <p>default=False. Defines whether to randomly mix-up the order of clonotypes before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up  clonotypes with the same size in read or UMIs.</p> <code>False</code> <code>count_threshold</code> <code>int</code> <p>limits [0:100000], all clonotypes with count less than this value will be filtered out</p> <code>None</code> <code>seed</code> <code>any hashable type</code> <p>better to use int - seed for reproducibility of random events  (downsampling or top with mix-tails). Default=None.</p> <code>None</code> Source code in <code>repseq/clone_filter.py</code> <pre><code>class Filter:\n\n    \"\"\"\n    Clonoset filter.\n    May be used to filter clonosets:\n        - by clone functionality\n        - randomly downsample them to particular number of reads of UMIs\n        - take top clonotypes by size (number of reads of UMIs) with or without random mixing\n\n    Args:\n        name (str): the name of the filter. Will be displayed in print\n        functionality (str): Possible values:\n            - \"a\" - any (default). No clones are filtered out\n            - \"f\" - only functional. Those, not having stop codons and \n                frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid\n                sequence\n            - \"n\" - only-nonfunctional - opposite to \"f\" - functional\n        downsample (int): the number of reads/UMIs to randomly downsample the clonoset to.\n            default value 'None' - means not to apply downsampling\n        top (int): the number of top biggest by reads/UMIs clonotypes to take from the clonoset.\n            default value 'None' - means not to apply top\n        by_umi (bool): default=False. Which column to take for clonotype count - reads or UMIs \n            (if UMI count column exists).\n        mix_tails (bool): default=False. Defines whether to randomly mix-up the order of clonotypes\n            before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up \n            clonotypes with the same size in read or UMIs.\n        count_threshold (int): limits [0:100000], all clonotypes with count less than this value will\n            be filtered out\n        seed (any hashable type): better to use int - seed for reproducibility of random events \n            (downsampling or top with mix-tails). Default=None.\n    \"\"\"\n\n    def __init__(self, name=\"default_filter\", functionality=\"a\", downsample=None,\n                 top=None, by_umi=False, mix_tails=False, count_threshold=None, \n                 unweight=False, seed=None, recount_fractions=True,\n                 white_list=[], black_list=[], pool_clonoset_by=\"\", convert=True, \n                 ignore_small_clonosets=False):\n        self.name = name\n        self.functionality = functionality\n        self.downsample_size = downsample\n        self.top = top\n        self.by_umi = by_umi\n        self.mix_tails = mix_tails\n        self.seed = seed\n        self.count_threshold = count_threshold\n        self.unweight = unweight\n        self.recount_fractions = recount_fractions\n        self.white_list = white_list\n        self.black_list = black_list\n        self.pool_by = pool_clonoset_by\n        self.convert = convert\n        self.ignore_small_clonosets = ignore_small_clonosets\n        self._check_input()\n\n    def spawn(self):\n        \"\"\"\n\n        Returns:\n            the copy of the filter. Necessary for parallel computing\n\n        \"\"\"\n        return Filter(name=self.name, functionality=self.functionality,\n                      downsample=self.downsample_size, top=self.top,\n                      by_umi=self.by_umi, mix_tails=self.mix_tails,\n                      count_threshold=self.count_threshold, seed=self.seed,\n                      unweight=self.unweight,\n                      recount_fractions=self.recount_fractions,\n                      white_list = self.white_list,\n                      black_list = self.black_list\n                      )\n\n    def apply(self, input_clonoset, colnames=None):\n        \"\"\"\n        Main method of the Filter object - application of it to a clonoset\n\n        Args:\n            input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n                MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n            colnames (dict, optional): Dictionary of available specific column names.\n                Defaults to None - colnames imputed automatically.\n\n        Returns:\n            clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n                format and applying functionality filtration and downsampling or taking top\n        \"\"\"\n\n        # copy clonoset for not changing the original one\n        clonoset = input_clonoset.copy()\n        if colnames is None:\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # create common columns: vdj-refPoints and VDJC-segments in common state\n        clonoset = self._make_common_columns(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n        # converting to common VDJtools-like format and obtaining new colnames\n        if self.convert:\n            clonoset = self._convert_clonoset(clonoset, colnames)\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # application of main filters\n        if self.functionality != \"a\":\n            clonoset = self._filter_by_functionality(clonoset, colnames)\n\n        clonoset = self._filter_by_count(clonoset, colnames)\n\n        clonoset = self._downsample(clonoset, colnames)\n        clonoset = self._get_top(clonoset, colnames)\n\n        if self.unweight:\n            clonoset = self._unweight(clonoset, colnames)\n        # the fraction columns need to be recounted after filtering, as they\n        # remain the same as in the original clonoset before filtration\n        if self.recount_fractions:\n            clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n        if self.pool_by:\n            clonoset = self._pool_clonoset(clonoset, colnames)\n        if len(self.white_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n        if len(self.black_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n        return clonoset\n\n    def _convert_clonoset(self, clonoset, colnames):\n        # copy clonoset for not changing the original one\n        c_clonoset = clonoset.copy()\n\n        # basic column name in clonoset DF\n\n        count_column, fraction_column = decide_count_and_frac_columns(colnames, self.by_umi, suppress_warnings=True)\n\n        rename_dict = {count_column: \"count\",\n                       fraction_column: \"freq\",\n                       colnames[\"cdr3aa_column\"]: \"cdr3aa\",\n                       colnames[\"cdr3nt_column\"]: \"cdr3nt\",\n                       colnames[\"v_column\"]: \"v\",\n                       colnames[\"d_column\"]: \"d\",\n                       colnames[\"j_column\"]: \"j\"}\n        c_clonoset = c_clonoset.rename(columns=rename_dict)\n\n        result_columns = [\"count\", \"freq\"]\n        if \"cdr3nt\" in c_clonoset.columns:\n            result_columns.append(\"cdr3nt\")\n        result_columns += [\"cdr3aa\", \"v\"]\n        segment_borders_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n\n\n\n\n        # In the case of MiXCR and Bioadaptive format the segment type columns\n        # usually show several segment variants with particular allele and score.\n        # Here we extract only the name of the best hit without allele ane score\n        c_clonoset[\"v\"] = c_clonoset[\"v\"].apply(lambda x: extract_segment(x))\n        if \"d\" in c_clonoset.columns:\n            c_clonoset[\"d\"] = c_clonoset[\"d\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"d\")\n        if \"j\" in c_clonoset.columns:\n            c_clonoset[\"j\"] = c_clonoset[\"j\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"j\")\n\n        # add the column for Constant segment if it exists in the original clonoset\n        if colnames[\"c_column\"] is not None:\n            c_clonoset = c_clonoset.rename(columns={colnames[\"c_column\"]: \"c\"})\n            c_clonoset[\"c\"] = c_clonoset[\"c\"].apply(lambda x: extract_segment(x))\n            result_columns += [\"c\"]\n\n        # obtain the borders of the segments within CDR3 region, if possible and add them to\n        # resulting clonoset\n        if \"refPoints\" in c_clonoset.columns:\n            c_clonoset[\"VEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n            c_clonoset[\"DStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n            c_clonoset[\"DEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n            c_clonoset[\"JStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        result_columns += [col for col in segment_borders_columns if col in c_clonoset.columns]    \n\n        # save \"sample_id\" column if it is present in clonoset\n        if \"sample_id\" in c_clonoset.columns:\n            result_columns.append(\"sample_id\")\n        c_clonoset = c_clonoset.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n        return c_clonoset[result_columns]\n\n    def _make_common_columns(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        # treat refPoints\n        refpoints_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n        if len(set(clonoset.columns).intersection(set(refpoints_columns))) &lt; 4: # check if not all the columns present\n            if \"refPoints\" in clonoset.columns: # if refPoints is present, create new columns\n                clonoset[\"VEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n                clonoset[\"DStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n                clonoset[\"DEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n                clonoset[\"JStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        # treat v,d,j segments\n\n        # V\n        if \"v\" not in clonoset.columns:\n            if colnames[\"v_column\"] is not None:\n                clonoset[\"v\"] = clonoset[colnames[\"v_column\"]]\n        if \"v\" in clonoset.columns:\n            clonoset[\"v\"] = clonoset[\"v\"].apply(lambda x: extract_segment(x))\n\n        # D\n        if \"d\" not in clonoset.columns:\n            if colnames[\"d_column\"] is not None:\n                clonoset[\"d\"] = clonoset[colnames[\"d_column\"]]\n        if \"d\" in clonoset.columns:\n            clonoset[\"d\"] = clonoset[\"d\"].apply(lambda x: extract_segment(x))\n\n        # J\n        if \"j\" not in clonoset.columns:\n            if colnames[\"j_column\"] is not None:\n                clonoset[\"j\"] = clonoset[colnames[\"j_column\"]]\n        if \"j\" in clonoset.columns:\n            clonoset[\"j\"] = clonoset[\"j\"].apply(lambda x: extract_segment(x))\n\n        # C\n        if \"c\" not in clonoset.columns:\n            if colnames[\"c_column\"] is not None:\n                clonoset[\"c\"] = clonoset[colnames[\"c_column\"]]\n        if \"c\" in clonoset.columns:\n            clonoset[\"c\"] = clonoset[\"c\"].apply(lambda x: extract_segment(x))\n\n        if \"cdr3aa\" not in clonoset.columns:\n            if colnames[\"cdr3aa_column\"] is not None:\n                clonoset[\"cdr3aa\"] = clonoset[colnames[\"cdr3aa_column\"]]\n        if \"cdr3nt\" not in clonoset.columns:\n            if colnames[\"cdr3nt_column\"] is not None:\n                clonoset[\"cdr3nt\"] = clonoset[colnames[\"cdr3nt_column\"]]\n\n        return clonoset\n\n\n\n    def _unweight(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n        clonoset[colnames[\"count_column\"]] = 1\n        return clonoset\n\n    def _recount_fractions_for_clonoset(self, clonoset_in, colnames):\n        if self.is_empty():\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n\n        count_column = colnames[\"count_column\"]\n        fraction_column = colnames[\"fraction_column\"]\n        umi_column = colnames[\"umi_column\"]\n        umi_fraction_column = colnames[\"umi_fraction_column\"]\n\n        clonoset[fraction_column] = clonoset[count_column]/clonoset[count_column].sum()\n        if colnames[\"umi\"]:\n            clonoset[umi_fraction_column] = clonoset[umi_column]/clonoset[umi_column].sum()\n        return clonoset\n\n    def _filter_by_count(self, clonoset_in, colnames):\n        if self.count_threshold is None:\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        clonoset = clonoset.loc[clonoset[count_column] &gt;= self.count_threshold]\n\n        return clonoset\n\n    def _check_input(self):\n\n        \"\"\"\n        Check if the object was created properly\n\n        Raises:\n            ValueError: in case of incorrect parameter values\n        \"\"\"\n        functionality_options = [\"a\", \"f\", \"n\"]\n        count_threshold_limits = [0, 100000]\n        if self.functionality not in functionality_options:\n            raise ValueError(f\"Incorrect value '{self.functionality}' for functionality. Possible values: {', '.join(functionality_options)}\")\n        if self.count_threshold is not None:\n            if not isinstance(self.count_threshold, int):\n                raise TypeError(\"Count threshold must be an 'int' or 'None'\")\n            if (self.count_threshold &lt; count_threshold_limits[0]\n                  or self.count_threshold &gt; count_threshold_limits[1]):\n                raise ValueError(f\"Incorrect value '{self.functionality}' for count_threshold. Possible values: {count_threshold_limits}\")                \n        if self.downsample_size is not None:\n            if not isinstance(self.downsample_size, int):\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Only int or None possible\")\n            elif self.downsample_size &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Value too low\")\n        if self.top is not None:\n            if not isinstance(self.top, int):\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Only int or None possible\")\n            elif self.top &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Value too low\")\n        if not isinstance(self.seed, Hashable):\n            raise ValueError(f\"Incorrect value '{self.seed}' for seed. Must be hashable\")\n        pool_by_options = [\"\", \"aa\", \"aaV\", \"aaVj\", \"nt\", \"ntV\", \"ntVJ\"]\n        if self.pool_by not in pool_by_options:\n            raise ValueError(f\"Incorrect value '{self.pool_by}' for clonoset pool. Possible values: {', '.join(pool_by_options)}\")\n\n    def _downsample(self, clonoset_in, colnames):\n        \"\"\"\n        Downsample clonoset.\n\n        This function takes the total number of reads or UMIs of the clonoset.\n        Then randomly samples the downsample_size from 0 to this total number of reads/UMIs. \n        This random sample is mapped to the clonotype sizes and \n        the new downsampled clonoset is created\n        \"\"\"\n\n        if self.downsample_size is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        total_count = int(clonoset[count_column].sum())\n\n        # raise ValueError if UMI/read count is less then downsample_size\n\n        if total_count &lt; self.downsample_size:\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"total count {total_count} is less than downsample size {self.downsample_size}\")\n        elif total_count == self.downsample_size:\n            return clonoset\n\n        # set seed if given and take the sample of total_count\n        if self.seed is not None:\n            random.seed(self.seed)\n        sample = sorted(random.sample(range(total_count), self.downsample_size))\n\n        # map the sample to the clone counts in the clonoset\n        curr_sum = 0\n        i = 0\n        new_counts_dict = {}\n        for index,r in clonoset.iterrows():\n            curr_sum+=r[count_column]\n            new_count = 0\n            if i == self.downsample_size:\n                break\n            while(sample[i]&lt;curr_sum):\n                new_count+=1\n                i+=1\n                if i == self.downsample_size:\n                    break\n            if new_count &gt; 0:\n                new_counts_dict[index]=new_count\n\n        # filter clonoset for missed clones and set new clone counts\n        (indices,counts) = zip(*new_counts_dict.items())\n        clonoset = clonoset.loc[clonoset.index.isin(indices)]\n        clonoset[count_column] = counts    \n        return clonoset.reset_index(drop=True)\n\n    def _get_top(self, clonoset_in, colnames):\n        \"\"\"\n        Takes top N biggest clones from the clonoset.\n\n        Mix-tails is recommended for use, because the order of the clonotypes\n        with equal count may not be independent from their other properties.\n        This option mixes up the order of all clonotypes in clonoset and then\n        sorts them by count in decreasing order, so that clonotypes with the same\n        count not have completely random order. Also use seed option for reproducibility\n        of the results.\n\n        Raises:\n            ValueError: if clone count is less then required top\n        \"\"\"\n\n        if self.top is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n\n        #shuffle the order of clonotypes if required\n        if self.seed is not None:\n            random.seed(self.seed)\n        if self.mix_tails:\n            index_order = random.sample(list(clonoset.index), len(clonoset))\n            clonoset = clonoset.iloc[index_order] \n            clonoset = clonoset.sort_values(by=count_column, ascending=False)\n\n        if self.top &gt; len(clonoset):\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"Warning! Clonoset size - {len(clonoset)} - is less than required top - {self.top}\")\n\n        # take top\n        if self.top &gt; 0:\n            clonoset=clonoset.iloc[:self.top]\n\n        return clonoset.reset_index(drop=True)\n\n    def _filter_by_functionality(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        if self.functionality == \"f\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=True)\n        if self.functionality == \"n\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=False)\n\n        return clonoset.reset_index(drop=True)\n\n    def __str__(self):\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"Filter name:\\t{self.name}\\n\"\n        output += f\"Functionality:\\t{functionality[self.functionality]}\\n\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"Count threshold:\\t{self.count_threshold}\\n\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"Downsample size:\\t{self.downsample_size}\\n\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"Take top:\\t{self.top}\\n\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"Clone size unweighted (all clone counts = 1)\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"Count by:\\t UMI (if exist)\\n\"\n            else:\n                output += f\"Count by:\\t reads/counts\\n\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"Seed for random:\\t{self.seed}\\n\"\n            else:\n                output += f\"Seed for random:\\tunset\\n\"\n                output += f\"Warning: filter contains random events.\\nTo obtain reproducible results, set seed in calcutations or manually (random.seed(some_int))\\nprior to applying the filter.\\nNote only seed that is set as this object parameter will work for mix_tails\\n\"\n\n        return output\n\n    def _pool_clonoset(self, clonoset_in, colnames):\n        # copy clonoset and sort by clone counts and reset index for order\n        clonoset = clonoset_in.copy().sort_values(by=colnames[\"count_column\"], ascending=False).reset_index(drop=True)\n\n        # create list of pool columns\n        aa, check_v, check_j = overlap_type_to_flags(self.pool_by)\n        columns_for_pool = []\n        if aa:\n            columns_for_pool.append(colnames[\"cdr3aa_column\"])\n        else:\n            columns_for_pool.append(colnames[\"cdr3nt_column\"])\n        if check_v:\n            columns_for_pool.append(colnames[\"v_column\"])\n        if check_j:\n            columns_for_pool.append(colnames[\"j_column\"])\n\n        # create column combining all pool columns\n        clonoset[\"pool_id\"] = clonoset.apply(lambda x: \"|\".join([x[colname] for colname in columns_for_pool]), axis=1)\n\n        indices_to_retain = []\n\n        for pool_id in clonoset[\"pool_id\"].unique():\n            pool_clonoset = clonoset.loc[clonoset[\"pool_id\"] == pool_id]\n\n            # select the clone with biggest count - it will represent pooled clonotypes by\n            # columns other that count and freq\n            top_index = pool_clonoset.index[0]\n            indices_to_retain.append(top_index)\n\n            # sum counts and fractions for pooled clonotypes\n            clonoset.loc[top_index,colnames[\"count_column\"]] = pool_clonoset[colnames[\"count_column\"]].sum()\n            clonoset.loc[top_index,colnames[\"fraction_column\"]] = pool_clonoset[colnames[\"fraction_column\"]].sum()\n\n        # retain only rows with representative clonotypes and remove technical column\n        clonoset = clonoset.loc[indices_to_retain].drop(columns=[\"pool_id\"])\n\n        return clonoset\n\n    def _filter_clonotypes(self, clonoset_in, list_type):\n        if list_type == \"white\":\n            clonotypes_list = self.white_list\n        elif list_type == \"black\":\n            clonotypes_list = self.black_list\n        else:\n            raise ValueError(\"list_type must be 'white' or 'black'\")\n\n        clonoset = clonoset_in.copy()\n\n        clonoset[\"filter_pass\"] = clonoset.apply(lambda x: self._compare_clonoset_list_row_with_clonotype(x, clonotypes_list), axis=1)\n        if list_type == \"white\":\n            clonoset = clonoset[clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        else:\n            clonoset = clonoset[~clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        return clonoset\n\n\n# def convert_clonoset_to_clonotype_filter_list(clonoset_df, overlap_type=\"aaVJ\"):\n#     clonotypes_list = []\n#     aa, include_v, include_j = intersections.overlap_type_to_flags(overlap_type)\n#     for i,r in clonoset_df.iterrows():\n#         clonotype = []\n#         if aa:\n#             clonotype.append(row[\"cdr3aa\"])\n#         else:\n#             clonotype.append(row[\"cdr3nt\"])\n#         if include_v:\n#             clonotype.append(row[\"v\"])\n#         if include_j:\n#             clonotype.append(row[\"j\"])\n\n#         clonotype = tuple(clonotype)\n#         clonotypes_list.append(clonotype)\n#     return clonotypes_list\n\n    def _compare_clonoset_row_with_clonotype(self, row, clonotype):\n        c_len = len(clonotype)\n        if c_len == 1:\n            if row[\"cdr3aa\"] == clonotype[0]:\n                return True\n        elif c_len == 2:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1]:\n                return True\n        elif c_len == 3:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1] and row[\"j\"] == clonotype[2]:\n                return True\n        else:\n            # need to write better explanation for error\n            raise ValueError(\"clonotypes must contain from 1 to 3 values\")\n\n        return False\n\n    def _compare_clonoset_list_row_with_clonotype(self, row, clonotypes_list):\n        for clonotype in clonotypes_list:\n            if self._compare_clonoset_row_with_clonotype(row, clonotype):\n                return True\n        return False\n\n    def is_empty(self):\n        return self.functionality == \"a\" and self.downsample_size is None and self.top is None and self.count_threshold is None and not self.unweight\n\n    def _repr_html_(self):\n        \"\"\"\n        function for printing the Filter properties to Jupyter output\n        \"\"\"\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"&lt;p&gt;Filter name: {self.name}&lt;/p&gt;\"\n        output += f\"&lt;p&gt;Functionality:\\t{functionality[self.functionality]}&lt;/p&gt;\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"&lt;p&gt;Count threshold: {self.count_threshold}&lt;/p&gt;\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"&lt;p&gt;Downsample size: {self.downsample_size}&lt;/p&gt;\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"&lt;p&gt;Take top: {self.top}&lt;/p&gt;\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"&lt;p&gt;Clone size unweighted: (all clone counts = 1)&lt;/p&gt;\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"&lt;p&gt;Count by:  UMI (if exist)&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Count by: reads/counts&lt;/p&gt;\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"&lt;p&gt;Seed for random: {self.seed}&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Seed for random:\\tunset&lt;/p&gt;\"\n                output += f\"&lt;p&gt;Warning: filter contains random events. To obtain reproducible results, set seed in calcutations or manually (random.seed(some_int)) prior to applying the filter. Note only seed that is set as this object parameter will work for mix_tails&lt;/p&gt;\"\n\n        return output\n</code></pre>"},{"location":"filter/#clone_filter.Filter.apply","title":"<code>apply(input_clonoset, colnames=None)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>input_clonoset</code> <code>DataFrame</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def apply(self, input_clonoset, colnames=None):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n    \"\"\"\n\n    # copy clonoset for not changing the original one\n    clonoset = input_clonoset.copy()\n    if colnames is None:\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # create common columns: vdj-refPoints and VDJC-segments in common state\n    clonoset = self._make_common_columns(clonoset, colnames)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    # converting to common VDJtools-like format and obtaining new colnames\n    if self.convert:\n        clonoset = self._convert_clonoset(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # application of main filters\n    if self.functionality != \"a\":\n        clonoset = self._filter_by_functionality(clonoset, colnames)\n\n    clonoset = self._filter_by_count(clonoset, colnames)\n\n    clonoset = self._downsample(clonoset, colnames)\n    clonoset = self._get_top(clonoset, colnames)\n\n    if self.unweight:\n        clonoset = self._unweight(clonoset, colnames)\n    # the fraction columns need to be recounted after filtering, as they\n    # remain the same as in the original clonoset before filtration\n    if self.recount_fractions:\n        clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n    if self.pool_by:\n        clonoset = self._pool_clonoset(clonoset, colnames)\n    if len(self.white_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n    if len(self.black_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n    return clonoset\n</code></pre>"},{"location":"filter/#clone_filter.Filter.spawn","title":"<code>spawn()</code>","text":"<p>Returns:</p> Type Description <p>the copy of the filter. Necessary for parallel computing</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def spawn(self):\n    \"\"\"\n\n    Returns:\n        the copy of the filter. Necessary for parallel computing\n\n    \"\"\"\n    return Filter(name=self.name, functionality=self.functionality,\n                  downsample=self.downsample_size, top=self.top,\n                  by_umi=self.by_umi, mix_tails=self.mix_tails,\n                  count_threshold=self.count_threshold, seed=self.seed,\n                  unweight=self.unweight,\n                  recount_fractions=self.recount_fractions,\n                  white_list = self.white_list,\n                  black_list = self.black_list\n                  )\n</code></pre>"},{"location":"functions/","title":"All functions","text":""},{"location":"functions/#clone_filter","title":"clone_filter","text":""},{"location":"functions/#clone_filter.Filter","title":"<code>Filter</code>","text":"<p>Clonoset filter. May be used to filter clonosets:     - by clone functionality     - randomly downsample them to particular number of reads of UMIs     - take top clonotypes by size (number of reads of UMIs) with or without random mixing</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the filter. Will be displayed in print</p> <code>'default_filter'</code> <code>functionality</code> <code>str</code> <p>Possible values: - \"a\" - any (default). No clones are filtered out - \"f\" - only functional. Those, not having stop codons and      frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid     sequence - \"n\" - only-nonfunctional - opposite to \"f\" - functional</p> <code>'a'</code> <code>downsample</code> <code>int</code> <p>the number of reads/UMIs to randomly downsample the clonoset to. default value 'None' - means not to apply downsampling</p> <code>None</code> <code>top</code> <code>int</code> <p>the number of top biggest by reads/UMIs clonotypes to take from the clonoset. default value 'None' - means not to apply top</p> <code>None</code> <code>by_umi</code> <code>bool</code> <p>default=False. Which column to take for clonotype count - reads or UMIs  (if UMI count column exists).</p> <code>False</code> <code>mix_tails</code> <code>bool</code> <p>default=False. Defines whether to randomly mix-up the order of clonotypes before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up  clonotypes with the same size in read or UMIs.</p> <code>False</code> <code>count_threshold</code> <code>int</code> <p>limits [0:100000], all clonotypes with count less than this value will be filtered out</p> <code>None</code> <code>seed</code> <code>any hashable type</code> <p>better to use int - seed for reproducibility of random events  (downsampling or top with mix-tails). Default=None.</p> <code>None</code> Source code in <code>repseq/clone_filter.py</code> <pre><code>class Filter:\n\n    \"\"\"\n    Clonoset filter.\n    May be used to filter clonosets:\n        - by clone functionality\n        - randomly downsample them to particular number of reads of UMIs\n        - take top clonotypes by size (number of reads of UMIs) with or without random mixing\n\n    Args:\n        name (str): the name of the filter. Will be displayed in print\n        functionality (str): Possible values:\n            - \"a\" - any (default). No clones are filtered out\n            - \"f\" - only functional. Those, not having stop codons and \n                frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid\n                sequence\n            - \"n\" - only-nonfunctional - opposite to \"f\" - functional\n        downsample (int): the number of reads/UMIs to randomly downsample the clonoset to.\n            default value 'None' - means not to apply downsampling\n        top (int): the number of top biggest by reads/UMIs clonotypes to take from the clonoset.\n            default value 'None' - means not to apply top\n        by_umi (bool): default=False. Which column to take for clonotype count - reads or UMIs \n            (if UMI count column exists).\n        mix_tails (bool): default=False. Defines whether to randomly mix-up the order of clonotypes\n            before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up \n            clonotypes with the same size in read or UMIs.\n        count_threshold (int): limits [0:100000], all clonotypes with count less than this value will\n            be filtered out\n        seed (any hashable type): better to use int - seed for reproducibility of random events \n            (downsampling or top with mix-tails). Default=None.\n    \"\"\"\n\n    def __init__(self, name=\"default_filter\", functionality=\"a\", downsample=None,\n                 top=None, by_umi=False, mix_tails=False, count_threshold=None, \n                 unweight=False, seed=None, recount_fractions=True,\n                 white_list=[], black_list=[], pool_clonoset_by=\"\", convert=True, \n                 ignore_small_clonosets=False):\n        self.name = name\n        self.functionality = functionality\n        self.downsample_size = downsample\n        self.top = top\n        self.by_umi = by_umi\n        self.mix_tails = mix_tails\n        self.seed = seed\n        self.count_threshold = count_threshold\n        self.unweight = unweight\n        self.recount_fractions = recount_fractions\n        self.white_list = white_list\n        self.black_list = black_list\n        self.pool_by = pool_clonoset_by\n        self.convert = convert\n        self.ignore_small_clonosets = ignore_small_clonosets\n        self._check_input()\n\n    def spawn(self):\n        \"\"\"\n\n        Returns:\n            the copy of the filter. Necessary for parallel computing\n\n        \"\"\"\n        return Filter(name=self.name, functionality=self.functionality,\n                      downsample=self.downsample_size, top=self.top,\n                      by_umi=self.by_umi, mix_tails=self.mix_tails,\n                      count_threshold=self.count_threshold, seed=self.seed,\n                      unweight=self.unweight,\n                      recount_fractions=self.recount_fractions,\n                      white_list = self.white_list,\n                      black_list = self.black_list\n                      )\n\n    def apply(self, input_clonoset, colnames=None):\n        \"\"\"\n        Main method of the Filter object - application of it to a clonoset\n\n        Args:\n            input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n                MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n            colnames (dict, optional): Dictionary of available specific column names.\n                Defaults to None - colnames imputed automatically.\n\n        Returns:\n            clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n                format and applying functionality filtration and downsampling or taking top\n        \"\"\"\n\n        # copy clonoset for not changing the original one\n        clonoset = input_clonoset.copy()\n        if colnames is None:\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # create common columns: vdj-refPoints and VDJC-segments in common state\n        clonoset = self._make_common_columns(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n        # converting to common VDJtools-like format and obtaining new colnames\n        if self.convert:\n            clonoset = self._convert_clonoset(clonoset, colnames)\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # application of main filters\n        if self.functionality != \"a\":\n            clonoset = self._filter_by_functionality(clonoset, colnames)\n\n        clonoset = self._filter_by_count(clonoset, colnames)\n\n        clonoset = self._downsample(clonoset, colnames)\n        clonoset = self._get_top(clonoset, colnames)\n\n        if self.unweight:\n            clonoset = self._unweight(clonoset, colnames)\n        # the fraction columns need to be recounted after filtering, as they\n        # remain the same as in the original clonoset before filtration\n        if self.recount_fractions:\n            clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n        if self.pool_by:\n            clonoset = self._pool_clonoset(clonoset, colnames)\n        if len(self.white_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n        if len(self.black_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n        return clonoset\n\n    def _convert_clonoset(self, clonoset, colnames):\n        # copy clonoset for not changing the original one\n        c_clonoset = clonoset.copy()\n\n        # basic column name in clonoset DF\n\n        count_column, fraction_column = decide_count_and_frac_columns(colnames, self.by_umi, suppress_warnings=True)\n\n        rename_dict = {count_column: \"count\",\n                       fraction_column: \"freq\",\n                       colnames[\"cdr3aa_column\"]: \"cdr3aa\",\n                       colnames[\"cdr3nt_column\"]: \"cdr3nt\",\n                       colnames[\"v_column\"]: \"v\",\n                       colnames[\"d_column\"]: \"d\",\n                       colnames[\"j_column\"]: \"j\"}\n        c_clonoset = c_clonoset.rename(columns=rename_dict)\n\n        result_columns = [\"count\", \"freq\"]\n        if \"cdr3nt\" in c_clonoset.columns:\n            result_columns.append(\"cdr3nt\")\n        result_columns += [\"cdr3aa\", \"v\"]\n        segment_borders_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n\n\n\n\n        # In the case of MiXCR and Bioadaptive format the segment type columns\n        # usually show several segment variants with particular allele and score.\n        # Here we extract only the name of the best hit without allele ane score\n        c_clonoset[\"v\"] = c_clonoset[\"v\"].apply(lambda x: extract_segment(x))\n        if \"d\" in c_clonoset.columns:\n            c_clonoset[\"d\"] = c_clonoset[\"d\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"d\")\n        if \"j\" in c_clonoset.columns:\n            c_clonoset[\"j\"] = c_clonoset[\"j\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"j\")\n\n        # add the column for Constant segment if it exists in the original clonoset\n        if colnames[\"c_column\"] is not None:\n            c_clonoset = c_clonoset.rename(columns={colnames[\"c_column\"]: \"c\"})\n            c_clonoset[\"c\"] = c_clonoset[\"c\"].apply(lambda x: extract_segment(x))\n            result_columns += [\"c\"]\n\n        # obtain the borders of the segments within CDR3 region, if possible and add them to\n        # resulting clonoset\n        if \"refPoints\" in c_clonoset.columns:\n            c_clonoset[\"VEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n            c_clonoset[\"DStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n            c_clonoset[\"DEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n            c_clonoset[\"JStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        result_columns += [col for col in segment_borders_columns if col in c_clonoset.columns]    \n\n        # save \"sample_id\" column if it is present in clonoset\n        if \"sample_id\" in c_clonoset.columns:\n            result_columns.append(\"sample_id\")\n        c_clonoset = c_clonoset.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n        return c_clonoset[result_columns]\n\n    def _make_common_columns(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        # treat refPoints\n        refpoints_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n        if len(set(clonoset.columns).intersection(set(refpoints_columns))) &lt; 4: # check if not all the columns present\n            if \"refPoints\" in clonoset.columns: # if refPoints is present, create new columns\n                clonoset[\"VEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n                clonoset[\"DStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n                clonoset[\"DEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n                clonoset[\"JStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        # treat v,d,j segments\n\n        # V\n        if \"v\" not in clonoset.columns:\n            if colnames[\"v_column\"] is not None:\n                clonoset[\"v\"] = clonoset[colnames[\"v_column\"]]\n        if \"v\" in clonoset.columns:\n            clonoset[\"v\"] = clonoset[\"v\"].apply(lambda x: extract_segment(x))\n\n        # D\n        if \"d\" not in clonoset.columns:\n            if colnames[\"d_column\"] is not None:\n                clonoset[\"d\"] = clonoset[colnames[\"d_column\"]]\n        if \"d\" in clonoset.columns:\n            clonoset[\"d\"] = clonoset[\"d\"].apply(lambda x: extract_segment(x))\n\n        # J\n        if \"j\" not in clonoset.columns:\n            if colnames[\"j_column\"] is not None:\n                clonoset[\"j\"] = clonoset[colnames[\"j_column\"]]\n        if \"j\" in clonoset.columns:\n            clonoset[\"j\"] = clonoset[\"j\"].apply(lambda x: extract_segment(x))\n\n        # C\n        if \"c\" not in clonoset.columns:\n            if colnames[\"c_column\"] is not None:\n                clonoset[\"c\"] = clonoset[colnames[\"c_column\"]]\n        if \"c\" in clonoset.columns:\n            clonoset[\"c\"] = clonoset[\"c\"].apply(lambda x: extract_segment(x))\n\n        if \"cdr3aa\" not in clonoset.columns:\n            if colnames[\"cdr3aa_column\"] is not None:\n                clonoset[\"cdr3aa\"] = clonoset[colnames[\"cdr3aa_column\"]]\n        if \"cdr3nt\" not in clonoset.columns:\n            if colnames[\"cdr3nt_column\"] is not None:\n                clonoset[\"cdr3nt\"] = clonoset[colnames[\"cdr3nt_column\"]]\n\n        return clonoset\n\n\n\n    def _unweight(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n        clonoset[colnames[\"count_column\"]] = 1\n        return clonoset\n\n    def _recount_fractions_for_clonoset(self, clonoset_in, colnames):\n        if self.is_empty():\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n\n        count_column = colnames[\"count_column\"]\n        fraction_column = colnames[\"fraction_column\"]\n        umi_column = colnames[\"umi_column\"]\n        umi_fraction_column = colnames[\"umi_fraction_column\"]\n\n        clonoset[fraction_column] = clonoset[count_column]/clonoset[count_column].sum()\n        if colnames[\"umi\"]:\n            clonoset[umi_fraction_column] = clonoset[umi_column]/clonoset[umi_column].sum()\n        return clonoset\n\n    def _filter_by_count(self, clonoset_in, colnames):\n        if self.count_threshold is None:\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        clonoset = clonoset.loc[clonoset[count_column] &gt;= self.count_threshold]\n\n        return clonoset\n\n    def _check_input(self):\n\n        \"\"\"\n        Check if the object was created properly\n\n        Raises:\n            ValueError: in case of incorrect parameter values\n        \"\"\"\n        functionality_options = [\"a\", \"f\", \"n\"]\n        count_threshold_limits = [0, 100000]\n        if self.functionality not in functionality_options:\n            raise ValueError(f\"Incorrect value '{self.functionality}' for functionality. Possible values: {', '.join(functionality_options)}\")\n        if self.count_threshold is not None:\n            if not isinstance(self.count_threshold, int):\n                raise TypeError(\"Count threshold must be an 'int' or 'None'\")\n            if (self.count_threshold &lt; count_threshold_limits[0]\n                  or self.count_threshold &gt; count_threshold_limits[1]):\n                raise ValueError(f\"Incorrect value '{self.functionality}' for count_threshold. Possible values: {count_threshold_limits}\")                \n        if self.downsample_size is not None:\n            if not isinstance(self.downsample_size, int):\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Only int or None possible\")\n            elif self.downsample_size &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Value too low\")\n        if self.top is not None:\n            if not isinstance(self.top, int):\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Only int or None possible\")\n            elif self.top &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Value too low\")\n        if not isinstance(self.seed, Hashable):\n            raise ValueError(f\"Incorrect value '{self.seed}' for seed. Must be hashable\")\n        pool_by_options = [\"\", \"aa\", \"aaV\", \"aaVj\", \"nt\", \"ntV\", \"ntVJ\"]\n        if self.pool_by not in pool_by_options:\n            raise ValueError(f\"Incorrect value '{self.pool_by}' for clonoset pool. Possible values: {', '.join(pool_by_options)}\")\n\n    def _downsample(self, clonoset_in, colnames):\n        \"\"\"\n        Downsample clonoset.\n\n        This function takes the total number of reads or UMIs of the clonoset.\n        Then randomly samples the downsample_size from 0 to this total number of reads/UMIs. \n        This random sample is mapped to the clonotype sizes and \n        the new downsampled clonoset is created\n        \"\"\"\n\n        if self.downsample_size is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        total_count = int(clonoset[count_column].sum())\n\n        # raise ValueError if UMI/read count is less then downsample_size\n\n        if total_count &lt; self.downsample_size:\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"total count {total_count} is less than downsample size {self.downsample_size}\")\n        elif total_count == self.downsample_size:\n            return clonoset\n\n        # set seed if given and take the sample of total_count\n        if self.seed is not None:\n            random.seed(self.seed)\n        sample = sorted(random.sample(range(total_count), self.downsample_size))\n\n        # map the sample to the clone counts in the clonoset\n        curr_sum = 0\n        i = 0\n        new_counts_dict = {}\n        for index,r in clonoset.iterrows():\n            curr_sum+=r[count_column]\n            new_count = 0\n            if i == self.downsample_size:\n                break\n            while(sample[i]&lt;curr_sum):\n                new_count+=1\n                i+=1\n                if i == self.downsample_size:\n                    break\n            if new_count &gt; 0:\n                new_counts_dict[index]=new_count\n\n        # filter clonoset for missed clones and set new clone counts\n        (indices,counts) = zip(*new_counts_dict.items())\n        clonoset = clonoset.loc[clonoset.index.isin(indices)]\n        clonoset[count_column] = counts    \n        return clonoset.reset_index(drop=True)\n\n    def _get_top(self, clonoset_in, colnames):\n        \"\"\"\n        Takes top N biggest clones from the clonoset.\n\n        Mix-tails is recommended for use, because the order of the clonotypes\n        with equal count may not be independent from their other properties.\n        This option mixes up the order of all clonotypes in clonoset and then\n        sorts them by count in decreasing order, so that clonotypes with the same\n        count not have completely random order. Also use seed option for reproducibility\n        of the results.\n\n        Raises:\n            ValueError: if clone count is less then required top\n        \"\"\"\n\n        if self.top is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n\n        #shuffle the order of clonotypes if required\n        if self.seed is not None:\n            random.seed(self.seed)\n        if self.mix_tails:\n            index_order = random.sample(list(clonoset.index), len(clonoset))\n            clonoset = clonoset.iloc[index_order] \n            clonoset = clonoset.sort_values(by=count_column, ascending=False)\n\n        if self.top &gt; len(clonoset):\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"Warning! Clonoset size - {len(clonoset)} - is less than required top - {self.top}\")\n\n        # take top\n        if self.top &gt; 0:\n            clonoset=clonoset.iloc[:self.top]\n\n        return clonoset.reset_index(drop=True)\n\n    def _filter_by_functionality(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        if self.functionality == \"f\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=True)\n        if self.functionality == \"n\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=False)\n\n        return clonoset.reset_index(drop=True)\n\n    def __str__(self):\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"Filter name:\\t{self.name}\\n\"\n        output += f\"Functionality:\\t{functionality[self.functionality]}\\n\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"Count threshold:\\t{self.count_threshold}\\n\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"Downsample size:\\t{self.downsample_size}\\n\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"Take top:\\t{self.top}\\n\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"Clone size unweighted (all clone counts = 1)\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"Count by:\\t UMI (if exist)\\n\"\n            else:\n                output += f\"Count by:\\t reads/counts\\n\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"Seed for random:\\t{self.seed}\\n\"\n            else:\n                output += f\"Seed for random:\\tunset\\n\"\n                output += f\"Warning: filter contains random events.\\nTo obtain reproducible results, set seed in calcutations or manually (random.seed(some_int))\\nprior to applying the filter.\\nNote only seed that is set as this object parameter will work for mix_tails\\n\"\n\n        return output\n\n    def _pool_clonoset(self, clonoset_in, colnames):\n        # copy clonoset and sort by clone counts and reset index for order\n        clonoset = clonoset_in.copy().sort_values(by=colnames[\"count_column\"], ascending=False).reset_index(drop=True)\n\n        # create list of pool columns\n        aa, check_v, check_j = overlap_type_to_flags(self.pool_by)\n        columns_for_pool = []\n        if aa:\n            columns_for_pool.append(colnames[\"cdr3aa_column\"])\n        else:\n            columns_for_pool.append(colnames[\"cdr3nt_column\"])\n        if check_v:\n            columns_for_pool.append(colnames[\"v_column\"])\n        if check_j:\n            columns_for_pool.append(colnames[\"j_column\"])\n\n        # create column combining all pool columns\n        clonoset[\"pool_id\"] = clonoset.apply(lambda x: \"|\".join([x[colname] for colname in columns_for_pool]), axis=1)\n\n        indices_to_retain = []\n\n        for pool_id in clonoset[\"pool_id\"].unique():\n            pool_clonoset = clonoset.loc[clonoset[\"pool_id\"] == pool_id]\n\n            # select the clone with biggest count - it will represent pooled clonotypes by\n            # columns other that count and freq\n            top_index = pool_clonoset.index[0]\n            indices_to_retain.append(top_index)\n\n            # sum counts and fractions for pooled clonotypes\n            clonoset.loc[top_index,colnames[\"count_column\"]] = pool_clonoset[colnames[\"count_column\"]].sum()\n            clonoset.loc[top_index,colnames[\"fraction_column\"]] = pool_clonoset[colnames[\"fraction_column\"]].sum()\n\n        # retain only rows with representative clonotypes and remove technical column\n        clonoset = clonoset.loc[indices_to_retain].drop(columns=[\"pool_id\"])\n\n        return clonoset\n\n    def _filter_clonotypes(self, clonoset_in, list_type):\n        if list_type == \"white\":\n            clonotypes_list = self.white_list\n        elif list_type == \"black\":\n            clonotypes_list = self.black_list\n        else:\n            raise ValueError(\"list_type must be 'white' or 'black'\")\n\n        clonoset = clonoset_in.copy()\n\n        clonoset[\"filter_pass\"] = clonoset.apply(lambda x: self._compare_clonoset_list_row_with_clonotype(x, clonotypes_list), axis=1)\n        if list_type == \"white\":\n            clonoset = clonoset[clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        else:\n            clonoset = clonoset[~clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        return clonoset\n\n\n# def convert_clonoset_to_clonotype_filter_list(clonoset_df, overlap_type=\"aaVJ\"):\n#     clonotypes_list = []\n#     aa, include_v, include_j = intersections.overlap_type_to_flags(overlap_type)\n#     for i,r in clonoset_df.iterrows():\n#         clonotype = []\n#         if aa:\n#             clonotype.append(row[\"cdr3aa\"])\n#         else:\n#             clonotype.append(row[\"cdr3nt\"])\n#         if include_v:\n#             clonotype.append(row[\"v\"])\n#         if include_j:\n#             clonotype.append(row[\"j\"])\n\n#         clonotype = tuple(clonotype)\n#         clonotypes_list.append(clonotype)\n#     return clonotypes_list\n\n    def _compare_clonoset_row_with_clonotype(self, row, clonotype):\n        c_len = len(clonotype)\n        if c_len == 1:\n            if row[\"cdr3aa\"] == clonotype[0]:\n                return True\n        elif c_len == 2:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1]:\n                return True\n        elif c_len == 3:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1] and row[\"j\"] == clonotype[2]:\n                return True\n        else:\n            # need to write better explanation for error\n            raise ValueError(\"clonotypes must contain from 1 to 3 values\")\n\n        return False\n\n    def _compare_clonoset_list_row_with_clonotype(self, row, clonotypes_list):\n        for clonotype in clonotypes_list:\n            if self._compare_clonoset_row_with_clonotype(row, clonotype):\n                return True\n        return False\n\n    def is_empty(self):\n        return self.functionality == \"a\" and self.downsample_size is None and self.top is None and self.count_threshold is None and not self.unweight\n\n    def _repr_html_(self):\n        \"\"\"\n        function for printing the Filter properties to Jupyter output\n        \"\"\"\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"&lt;p&gt;Filter name: {self.name}&lt;/p&gt;\"\n        output += f\"&lt;p&gt;Functionality:\\t{functionality[self.functionality]}&lt;/p&gt;\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"&lt;p&gt;Count threshold: {self.count_threshold}&lt;/p&gt;\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"&lt;p&gt;Downsample size: {self.downsample_size}&lt;/p&gt;\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"&lt;p&gt;Take top: {self.top}&lt;/p&gt;\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"&lt;p&gt;Clone size unweighted: (all clone counts = 1)&lt;/p&gt;\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"&lt;p&gt;Count by:  UMI (if exist)&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Count by: reads/counts&lt;/p&gt;\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"&lt;p&gt;Seed for random: {self.seed}&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Seed for random:\\tunset&lt;/p&gt;\"\n                output += f\"&lt;p&gt;Warning: filter contains random events. To obtain reproducible results, set seed in calcutations or manually (random.seed(some_int)) prior to applying the filter. Note only seed that is set as this object parameter will work for mix_tails&lt;/p&gt;\"\n\n        return output\n</code></pre>"},{"location":"functions/#clone_filter.Filter.apply","title":"<code>apply(input_clonoset, colnames=None)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>input_clonoset</code> <code>DataFrame</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def apply(self, input_clonoset, colnames=None):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n    \"\"\"\n\n    # copy clonoset for not changing the original one\n    clonoset = input_clonoset.copy()\n    if colnames is None:\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # create common columns: vdj-refPoints and VDJC-segments in common state\n    clonoset = self._make_common_columns(clonoset, colnames)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    # converting to common VDJtools-like format and obtaining new colnames\n    if self.convert:\n        clonoset = self._convert_clonoset(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # application of main filters\n    if self.functionality != \"a\":\n        clonoset = self._filter_by_functionality(clonoset, colnames)\n\n    clonoset = self._filter_by_count(clonoset, colnames)\n\n    clonoset = self._downsample(clonoset, colnames)\n    clonoset = self._get_top(clonoset, colnames)\n\n    if self.unweight:\n        clonoset = self._unweight(clonoset, colnames)\n    # the fraction columns need to be recounted after filtering, as they\n    # remain the same as in the original clonoset before filtration\n    if self.recount_fractions:\n        clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n    if self.pool_by:\n        clonoset = self._pool_clonoset(clonoset, colnames)\n    if len(self.white_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n    if len(self.black_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n    return clonoset\n</code></pre>"},{"location":"functions/#clone_filter.Filter.spawn","title":"<code>spawn()</code>","text":"<p>Returns:</p> Type Description <p>the copy of the filter. Necessary for parallel computing</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def spawn(self):\n    \"\"\"\n\n    Returns:\n        the copy of the filter. Necessary for parallel computing\n\n    \"\"\"\n    return Filter(name=self.name, functionality=self.functionality,\n                  downsample=self.downsample_size, top=self.top,\n                  by_umi=self.by_umi, mix_tails=self.mix_tails,\n                  count_threshold=self.count_threshold, seed=self.seed,\n                  unweight=self.unweight,\n                  recount_fractions=self.recount_fractions,\n                  white_list = self.white_list,\n                  black_list = self.black_list\n                  )\n</code></pre>"},{"location":"functions/#clonosets","title":"clonosets","text":""},{"location":"functions/#clonosets.find_all_exported_clonosets","title":"<code>find_all_exported_clonosets(folders, chain=None, remove_non_target=False, non_target_threshold=0.01)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>folders</code> <code>path or list of paths</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clonosets.py</code> <pre><code>def find_all_exported_clonosets(folders, chain=None, remove_non_target=False, non_target_threshold=0.01):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        folders (path or list of paths): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n\n    \"\"\"\n    if isinstance(folders, str):\n        folders = [folders]\n    clonosets_dfs = []\n    for folder in folders:\n        clonosets_dfs.append(find_all_exported_clonosets_in_folder(folder, chain=chain, remove_non_target=remove_non_target, non_target_threshold=non_target_threshold))\n    return pd.concat(clonosets_dfs)\n</code></pre>"},{"location":"functions/#clustering","title":"clustering","text":""},{"location":"functions/#clustering.Node","title":"<code>Node</code>","text":"Source code in <code>repseq/clustering.py</code> <pre><code>class Node:\n\n    def __init__(self, node_id, seq_nt, seq_aa, v, j, sample_id, size=1):\n        self.id = node_id\n        self.v = v\n        self.j = j\n        self.seq_aa = seq_aa\n        self.seq_nt = seq_nt\n        self.sample_id = sample_id\n        self.size = size\n        self.additional_properties = {}\n\n    def is_neighbour_of(self, other, mismatches=1, aa=True, check_v=False, check_j=False):\n        \"\"\"function compare two strings and return\n        True if their are equal\n            or if they have one mismatch and equal length\n        False in all other conditions\n        \"\"\"\n\n        if aa:\n            string1 = self.seq_aa\n            string2 = other.seq_aa\n        else:\n            string1 = self.seq_nt\n            string2 = other.seq_nt\n        if len(string1) != len(string2):\n             return False\n        if check_v and self.v != other.v:\n            return False\n        if check_j and self.j != other.j:\n            return False\n        hamm_dist = sum([a != b for a,b in zip(string1,string2)]) \n        if hamm_dist &gt; mismatches:\n            return False     \n        return True\n\n    def __str__(self):\n        return \"{}_{}\".format(self.seq_aa, self.id)\n\n    def add_properties(self, metadata):\n        if self.sample_id not in metadata:\n            for property in list(metadata[list(metadata)[0]].keys()):\n                self.additional_properties[property] = None\n        else:\n            self.additional_properties.update(metadata[self.sample_id])\n</code></pre>"},{"location":"functions/#clustering.Node.is_neighbour_of","title":"<code>is_neighbour_of(other, mismatches=1, aa=True, check_v=False, check_j=False)</code>","text":"<p>function compare two strings and return True if their are equal     or if they have one mismatch and equal length False in all other conditions</p> Source code in <code>repseq/clustering.py</code> <pre><code>def is_neighbour_of(self, other, mismatches=1, aa=True, check_v=False, check_j=False):\n    \"\"\"function compare two strings and return\n    True if their are equal\n        or if they have one mismatch and equal length\n    False in all other conditions\n    \"\"\"\n\n    if aa:\n        string1 = self.seq_aa\n        string2 = other.seq_aa\n    else:\n        string1 = self.seq_nt\n        string2 = other.seq_nt\n    if len(string1) != len(string2):\n         return False\n    if check_v and self.v != other.v:\n        return False\n    if check_j and self.j != other.j:\n        return False\n    hamm_dist = sum([a != b for a,b in zip(string1,string2)]) \n    if hamm_dist &gt; mismatches:\n        return False     \n    return True\n</code></pre>"},{"location":"functions/#common_functions","title":"common_functions","text":""},{"location":"functions/#constants","title":"constants","text":""},{"location":"functions/#diffexp","title":"diffexp","text":""},{"location":"functions/#intersections","title":"intersections","text":""},{"location":"functions/#intersections.find_intersecting_clonotypes","title":"<code>find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>The permissible number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: similar clonotypes by <code>overlap_type</code> in one particular clonoset are NOT combined into one and are treated as different clonotypes.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): The permissible number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset are NOT combined into one\n    and are treated as different clonotypes.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, strict=not bool(mismatches))\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))        \n    else:\n        for i in range(samples_total):\n            for j in range(samples_total):\n                sample1 = sample_list[i]\n                sample2 = sample_list[j]\n                if sample1 != sample2:\n                    tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(find_overlapping_clones_in_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    return pd.concat(result_list).reset_index(drop=True)\n</code></pre>"},{"location":"functions/#intersections.intersect_clones_in_samples_batch","title":"<code>intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type='aaV', by_freq=True, clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating frequencies of intersecting clonotypes between multiple repseq samples. The result of this function may be used for scatterplots of frequencies/counts of  overlapping clonotypes</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>by_umi</code> <code>bool</code> <p>set <code>=True</code> for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>True</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: when using particular overlap type, similar clonotypes in one particular clonoset are combined into one with summation of counts/frequencies.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code> clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type=\"aaV\", by_freq=True, clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating frequencies of intersecting clonotypes between multiple repseq samples.\n    The result of this function may be used for scatterplots of frequencies/counts of \n    overlapping clonotypes\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        by_umi (bool): set `=True` for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: when using particular overlap type, similar clonotypes in one particular clonoset are\n    combined into one with summation of counts/frequencies.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`\n            clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq,\n                                                                                                                        strict=True)\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists))\n    else:\n        for i in range(samples_total):\n            sample1 = sample_list[i]\n            for j in range(samples_total-i-1):\n                sample2 = sample_list[j+i+1]\n                tasks.append((sample1, sample2, clonoset_lists))\n\n    results = run_parallel_calculation(intersect_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    # df = pd.concat(results).index.set_names()\n    df = pd.concat(results).reset_index(drop=True)\n    df = split_tuple_clone_column(df, overlap_type)\n\n    return df\n</code></pre>"},{"location":"functions/#intersections.overlap_distances","title":"<code>overlap_distances(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>The permissible number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required similar clonotypes by <code>overlap_type</code> in one particular clonoset will be combined into one <p>clonotype with sum for count.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def overlap_distances(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): The permissible number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset will be combined into one\n        clonotype with sum for count.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    metric = metric.upper()\n    metrics = [\"F\", \"F2\", \"C\", \"BC\", \"J\", \"JSD\"]\n    mismatch_metrics = [\"F\", \"C\"]\n    non_symmetry_metrics = [\"C\"]\n    frequency_metrics = [\"F\", \"F2\", \"C\"]\n\n\n    if metric not in metrics:\n        raise ValueError(f\"Metric {metric} is not supported. Possible values: {', '.join(metrics)}\")\n\n    if mismatches and metric not in mismatch_metrics:\n        raise ValueError(f\"Metric {metric} does not allow mismatches. Mismatches only possible for: {', '.join(mismatch_metrics)}\")\n\n    by_freq = metric in frequency_metrics\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq)\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))        \n    else:\n        if metric not in non_symmetry_metrics and not two_dataframes:\n            for i in range(samples_total):\n                sample1 = sample_list[i]\n                for j in range(samples_total-i-1):\n                    sample2 = sample_list[j+i+1]\n                    tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n                if metric == \"F2\":\n                    tasks.append((sample1, sample1, clonoset_lists, mismatches, metric))\n        else:\n            for i in range(samples_total):\n                for j in range(samples_total):\n                    sample1 = sample_list[i]\n                    sample2 = sample_list[j]\n                    if sample1 != sample2:\n                        tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(overlap_metric_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    if not two_dataframes and metric != \"C\":\n        result_list = result_list + [(result[1], result[0], result[2]) for result in result_list]\n    overlap_df = pd.DataFrame(result_list, columns=[\"sample1\", \"sample2\", metric.lower()]).pivot_table(index=\"sample1\", columns=[\"sample2\"], values=metric.lower()).reset_index().set_index(\"sample1\").fillna(1)\n    return overlap_df\n</code></pre>"},{"location":"functions/#intersections.prepare_clonotypes_dfs_for_intersections","title":"<code>prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>description</p> required <code>clonosets_df2</code> <code>DataFrame</code> <p>description</p> required <code>cl_filter</code> <code>Filter</code> <p>description</p> required <code>cl_filter2</code> <code>Filter</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>description</p> required <code>by_freq</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>clonoset_lists</code> <code>dict</code> <p>dict of </p> <code>samples_total</code> <code>int</code> <code>two_dataframes</code> <code>bool</code> <code>sample_list</code> <code>list</code> <code>sample_list2</code> <code>list</code> Source code in <code>repseq/intersections.py</code> <pre><code>def prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False):\n    \"\"\"\n    Args:\n        clonosets_df (pd.DataFrame): _description_\n        clonosets_df2 (pd.DataFrame): _description_\n        cl_filter (Filter): _description_\n        cl_filter2 (Filter): _description_\n        overlap_type (str): _description_\n        by_freq (bool, optional): _description_. Defaults to True.\n\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n\n    Returns:\n        clonoset_lists (dict): dict of \n        samples_total (int): \n        two_dataframes (bool):\n        sample_list (list):\n        sample_list2 (list):\n    \"\"\"\n    # output:\n    ### clonoset_lists\n\n    if len(clonosets_df.sample_id.unique()) &lt; len(clonosets_df):\n        raise ValueError(\"Input clonosets in DataFrame have non-unique sample_id's\")\n    clonosets_df_1 = clonosets_df[[\"sample_id\", \"filename\"]]\n    two_dataframes = False\n    if isinstance(clonosets_df2, pd.DataFrame):\n        two_dataframes = True\n        if len(clonosets_df2.sample_id.unique()) &lt; len(clonosets_df2):\n            raise ValueError(\"Input clonosets in DataFrame2 have non-unique sample_id's\")\n        clonosets_df_2 = clonosets_df2[[\"sample_id\", \"filename\"]]\n        intersecting_sample_ids = set(clonosets_df2.sample_id.unique()).intersection(set(clonosets_df.sample_id.unique()))\n        if len(intersecting_sample_ids) &gt; 0 and cl_filter2 is not None:\n            print(\"WARNING! Some samples have the same sample_id in two sample_df's. The second filter will be applied to common samples\")\n\n\n    # converting clonosets to compact lists of clonotypes separated by CDR3 lengths to dictionary based on overlap type and count/freq/umi\n    clonoset_lists = convert_clonosets_to_compact_dicts(clonosets_df_1, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n    if two_dataframes:\n        if cl_filter2 is None:\n            cl_filter2 = cl_filter\n        clonoset_lists_2 = convert_clonosets_to_compact_dicts(clonosets_df_2, cl_filter=cl_filter2,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n        clonoset_lists.update(clonoset_lists_2)\n\n    samples_total = len(clonosets_df_1)\n    if two_dataframes:\n        samples_total = len(pd.concat([clonosets_df_1, clonosets_df_2]))\n\n    sample_list = list(clonosets_df_1.sort_values(by=\"sample_id\").sample_id)\n    sample_list2 = None\n    if two_dataframes:\n        sample_list2 = list(clonosets_df_2.sort_values(by=\"sample_id\").sample_id)\n\n    return clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2\n</code></pre>"},{"location":"functions/#io","title":"io","text":""},{"location":"functions/#io.open_json_report","title":"<code>open_json_report(filename)</code>","text":"<p>Supporting function for <code>read_json_report</code>. Reads the last record from json file.</p> Source code in <code>repseq/io.py</code> <pre><code>def open_json_report(filename):\n    \"\"\"\n    Supporting function for `read_json_report`. Reads the last record from json file.\n    \"\"\"\n\n    with open(filename) as data_file:    \n        for jsonObj in data_file:\n            report = json.loads(jsonObj)\n    return report\n</code></pre>"},{"location":"functions/#io.read_clonoset","title":"<code>read_clonoset(filename)</code>","text":"<p>Reads generic clonoset files.  Easyly reads <code>csv</code>, <code>tsv</code>, <code>txt</code> or <code>gz</code> files. Reads first found file inside <code>zip</code> files. Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to clonoset file</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>DataFrame representation of clonoset in given file. Bioadaptive clonosets are converted to vdjtools-like format.</p> Source code in <code>repseq/io.py</code> <pre><code>def read_clonoset(filename):\n    \"\"\"\n    Reads generic clonoset files. \n    Easyly reads `csv`, `tsv`, `txt` or `gz` files.\n    Reads first found file inside `zip` files.\n    Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive\n\n    Args:\n        filename (str): path to clonoset file\n\n    Returns:\n        clonoset (pd.DataFrame): DataFrame representation of clonoset in given file.\n            Bioadaptive clonosets are converted to vdjtools-like format.\n    \"\"\"\n\n\n    file_name, file_extension = os.path.splitext(filename)\n\n    d_types_mixcr = {'cloneId': int, 'readCount': int, 'readFraction': float,\n                    'uniqueUMICount': int, 'uniqueUMIFraction': float,\n                    'uniqueMoleculeCount': int, 'uniqueMoleculeFraction': float,\n                    'cloneCount': int, 'cloneFraction': float,\n                    'targetSequences': str, 'targetQualities': str,\n                    'allVHitsWithScore': str, 'allDHitsWithScore': str,\n                    'allJHitsWithScore': str, 'allCHitsWithScore': str,\n                    'allVAlignments': str, 'allDAlignments': str,\n                    'allJAlignments': str, 'allCAlignments': str,\n                    'nSeqFR1': str, 'minQualFR1': str,\n                    'nSeqCDR1': str, 'minQualCDR1': str,\n                    'nSeqFR2': str, 'minQualFR2': str,\n                    'nSeqCDR2': str, 'minQualCDR2': str,\n                    'nSeqFR3': str, 'minQualFR3': str,\n                    'nSeqCDR3': str, 'minQualCDR3': str,\n                    'nSeqFR4': str, 'minQualFR4': str,\n                    'aaSeqFR1': str, 'aaSeqCDR1': str,\n                    'aaSeqFR2': str, 'aaSeqCDR2': str,\n                    'aaSeqFR3': str, 'aaSeqCDR3': str,\n                    'aaSeqFR4': str, 'refPoints': str\n                    }\n\n    d_types_vdjtools = {'cdr3aa': str, 'cdr3nt': str,\n                        'v': str, 'd': str, 'j': str,\n                        'CDR3aa': str, 'CDR3nt': str,\n                        'V': str, 'D': str, 'J': str,\n                        'C': str, \"frequency\": float#,\n                        #'count': int, 'freq': float#,\n                        #'VEnd':int, 'DStart':int, 'DEnd':int, \"JStart\":int\n                        }\n\n    d_types_bioadaptive = {'nucleotide': str, 'aminoAcid': str,\n                            'count (templates/reads)': int,\n                            'frequencyCount (%)': float,\n                            'vGeneName': str, 'dGeneName': str,\n                            'jGeneName': str, 'cdr3Length': int,\n                            'n1Index': int,'dIndex': int,\n                            'n2Index': int,'jIndex': int\n                            }\n\n\n    datatypes = {**d_types_mixcr,**d_types_vdjtools, **d_types_bioadaptive}\n    if file_extension == \".zip\":\n        archive = zipfile.ZipFile(filename, 'r')\n        inner_filename = zipfile.ZipFile.namelist(archive)[0]\n        filename = archive.open(inner_filename)\n    clonoset = pd.read_csv(filename, sep=\"\\t\", dtype=datatypes)\n    if 'count (templates/reads)' in clonoset.columns:\n        clonoset = convert_bioadaptive_clonoset(clonoset)\n    return clonoset\n</code></pre>"},{"location":"functions/#io.read_json_report","title":"<code>read_json_report(sample_id, folder, report_type)</code>","text":"<p>Reads MiXCR4 json reports into a Python mixed data structure. This function takes the last json record, if for example MiXCR adds up several records  to json file (it happens, when the program is rerun several times on the same data). Program also includes cases when Sample-barcodes are used.</p> <p>Parameters:</p> Name Type Description Default <code>sample_id</code> <code>str</code> <p>sample_id used when running the MiXCR program</p> required <code>folder</code> <code>str</code> <p>folder in which the MiXCR output is stored</p> required <code>report_type</code> <code>str</code> <p>align, refine, assemble</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>mixed dict/list python structure, representing the json report</p> Source code in <code>repseq/io.py</code> <pre><code>def read_json_report(sample_id, folder, report_type):\n    \"\"\"\n    Reads MiXCR4 json reports into a Python mixed data structure.\n    This function takes the last json record, if for example MiXCR adds up several records \n    to json file (it happens, when the program is rerun several times on the same data).\n    Program also includes cases when Sample-barcodes are used.\n\n    Args:\n        sample_id (str): sample_id used when running the MiXCR program\n        folder (str): folder in which the MiXCR output is stored\n        report_type (str): align, refine, assemble\n\n    Returns:\n        report (dict): mixed dict/list python structure, representing the json report\n    \"\"\"\n\n\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    if \".\" in sample_id:\n        sample_id2 = \".\".join(sample_id.split(\".\")[:-1])\n        filename2 = os.path.join(folder, f\"{sample_id2}.{report_type}.report.json\")\n        try:\n            report = open_json_report(filename)\n        except FileNotFoundError:\n            report = open_json_report(filename2)\n    else:\n        report = open_json_report(filename)\n    return report\n</code></pre>"},{"location":"functions/#io.read_yaml_metadata","title":"<code>read_yaml_metadata(folder, filename='metadata.yaml')</code>","text":"<p>Reads NGSiK metadata from a given folder and converts to <code>pd.DataFrame</code>. By default  it searches for <code>metadata.yaml</code> file in this folder and extracts the table.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to NGSiK folder</p> required <code>filename</code> <code>str</code> <p>NGSiK metadata filename</p> <code>'metadata.yaml'</code> <p>Returns:</p> Name Type Description <code>sample_df</code> <code>DataFrame</code> <p>extracted DataFrame from metadata</p> Source code in <code>repseq/io.py</code> <pre><code>def read_yaml_metadata(folder, filename=\"metadata.yaml\"):\n\n    \"\"\"\n    Reads NGSiK metadata from a given folder and converts to `pd.DataFrame`. By default \n    it searches for `metadata.yaml` file in this folder and extracts the table.\n\n    Args:\n        folder (str): path to NGSiK folder\n        filename (str): NGSiK metadata filename\n\n    Returns:\n        sample_df (pd.DataFrame): extracted DataFrame from metadata\n\n    \"\"\"\n\n\n    most_important_columns = [\"sample_id\", \"R1\", \"R2\",\"libraryPerson\", \"projectPerson\", \"projectName\", \"species\", \"miNNNPattern\", \"SMPL\", \"mix_id\", \"preset\", \"startingMaterial\", \"libraryType\"]\n    yaml_filename = os.path.join(folder, filename)\n    with open(yaml_filename, \"r\") as stream:\n        try:\n            metadata_dict =yaml.safe_load(stream)\n    #         pd.io.json.json_normalize(metadata_dict, \"file\", \"samples\", errors='ignore')\n        except yaml.YAMLError as exc:\n            print(exc)\n\n    df = pd.json_normalize(metadata_dict)\n    df = df.explode(\"file\")\n    df = pd.concat([df.drop(['file'], axis=1), df['file'].apply(pd.Series)], axis=1)\n    df = df.explode(\"samples\")\n    df = pd.concat([df.drop(['samples'], axis=1), df['samples'].apply(pd.Series)], axis=1)\n    if 'patternGroupValues' in df.columns:\n        df = pd.concat([df.drop(['patternGroupValues'], axis=1), df['patternGroupValues'].apply(pd.Series)], axis=1)\n    df[\"R1\"] = df[\"R1\"].apply(lambda x: os.path.join(folder, x))\n    df[\"R2\"] = df[\"R2\"].apply(lambda x: os.path.join(folder, x))\n    df = df.rename(columns={\"name\": \"sample_id\"})\n\n    for col_name in most_important_columns[::-1]:\n        if col_name in df.columns:\n            first_column = df.pop(col_name) \n            df.insert(0, col_name, first_column)\n\n    return df.reset_index(drop=True)\n</code></pre>"},{"location":"functions/#logo","title":"logo","text":""},{"location":"functions/#migec","title":"migec","text":""},{"location":"functions/#minnn","title":"minnn","text":""},{"location":"functions/#mixcr","title":"mixcr","text":""},{"location":"functions/#mixcr.get_processing_table","title":"<code>get_processing_table(folder, show_offtarget=False, off_target_chain_threshold=0.01)</code>","text":"<p>Searches for clonosets in the the folder, extracts their sample_id's and shows main processing stats in a table format. By default does not show \"off-target\" clonosets -  those having less than 1% (default, may be overriden) of reads for the sample_id. For example, you have sequenced TRB sample, but there is found 0.5% (by read count)  of TRA chains for the same sample_id, then the clonoset will not be shown in the table. You can specify <code>show_offtarget=True</code> to display all found chains in the table or  outherwise set a higher value for <code>off_target_chain_threshold</code> (<code>0.01</code> by default).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str or list</code> <p>folder or list of folders in which to look for clonosets and processing stats</p> required <code>show_offtarget</code> <code>bool</code> <p>add offtarget chains to the stats</p> <code>False</code> <code>off_target_chain_threshold</code> <code>float</code> <p>threshold for off-target chains</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe, containing <code>sample_id</code>, <code>extracted_chain</code> and  different processing stats columns. There may be several rows with the same  <code>sample_id</code>, with each found <code>extracted_chain</code></p> Source code in <code>repseq/mixcr.py</code> <pre><code>def get_processing_table(folder, show_offtarget=False, off_target_chain_threshold=0.01):\n    \"\"\"\n    Searches for clonosets in the the folder, extracts their sample_id's and shows main\n    processing stats in a table format. By default does not show \"off-target\" clonosets - \n    those having less than 1% (default, may be overriden) of reads for the sample_id.\n    For example, you have sequenced TRB sample, but there is found 0.5% (by read count) \n    of TRA chains for the same sample_id, then the clonoset will not be shown in the table.\n    You can specify `show_offtarget=True` to display all found chains in the table or \n    outherwise set a higher value for `off_target_chain_threshold` (`0.01` by default).\n\n    Args:\n        folder (str or list): folder or list of folders in which to look for clonosets and\n            processing stats\n        show_offtarget (bool): add offtarget chains to the stats\n        off_target_chain_threshold (float): threshold for off-target chains\n\n    Returns:\n        df (pd.DataFrame): dataframe, containing `sample_id`, `extracted_chain` and \n            different processing stats columns. There may be several rows with the same \n            `sample_id`, with each found `extracted_chain`\n    \"\"\"\n\n    if isinstance(folder, list):\n        tables = []\n        for f in folder:\n            table = get_processing_table(f, show_offtarget=show_offtarget)\n            tables.append(table)\n        return pd.concat(tables).sort_values(by=\"sample_id\").reset_index(drop=True)\n\n    results = []\n    clonosets = find_all_exported_clonosets_in_folder(folder, chain=None)\n\n    for i, r in clonosets.iterrows():\n        sample_id = r[\"sample_id\"]\n        chain = r[\"chain\"]\n        align_report = read_json_report(sample_id, folder, \"align\")\n\n        try:\n            refine_report = read_json_report(sample_id, folder, \"refine\")\n            umi = True\n        except FileNotFoundError:\n            umi = False\n\n        assemble_report = read_json_report(sample_id, folder, \"assemble\")\n\n        # print(sample_id, chain)\n        clonoset = read_clonoset(r.filename)\n        clonoset_f = filter_nonfunctional_clones(clonoset)\n\n        # align report\n        Rt=align_report[\"totalReadsProcessed\"]\n        Ru=align_report[\"totalReadsProcessed\"]-align_report[\"notAlignedReasons\"][\"NoBarcode\"]\n        Ru_pc = round(Ru/Rt*100, 2)\n        Ra=align_report[\"aligned\"]\n        Ra_pc = round(Ra/Rt*100, 2)\n        Roa = align_report[\"overlappedAligned\"]\n        Roa_pc = round(Roa/Ra*100, 2)\n\n        if umi:\n        #Ra2=refine_report[\"correctionReport\"][\"inputRecords\"] ##### differs from Ra, but D.Bolotin did not explain why\n\n            UMIa=refine_report[\"correctionReport\"][\"steps\"][0][\"inputDiversity\"]\n            UMIc=refine_report[\"correctionReport\"][\"steps\"][0][\"outputDiversity\"]\n            try:\n                UMIf=refine_report[\"correctionReport\"][\"filterReport\"][\"numberOfGroupsAccepted\"]\n            except TypeError:\n                UMIf=UMIc\n            Rf=refine_report[\"correctionReport\"][\"outputRecords\"]\n            try:\n                overseq_threshold = int(refine_report[\"correctionReport\"][\"filterReport\"][\"operatorReports\"][0][\"operatorReport\"][\"threshold\"])\n            except TypeError:\n                overseq_threshold = None\n            reads_per_umi = round(Rf/UMIf, 2)\n        else:\n            UMIa = np.nan\n            UMIc = np.nan\n            UMIf = np.nan\n            Rf = np.nan\n            overseq_threshold = np.nan\n            reads_per_umi = np.nan\n\n        Ct=assemble_report[\"clones\"]\n        Rcl=assemble_report[\"readsInClones\"]\n\n        Ctc=len(clonoset)\n        Rclc=int(clonoset.readCount.sum())\n\n        Cfunc=len(clonoset_f)\n        Rfunc=int(clonoset_f.readCount.sum())\n        if umi:\n            UMIcl=clonoset.uniqueMoleculeCount.sum()\n            UMIfunc=clonoset_f.uniqueMoleculeCount.sum()\n        else:\n            UMIcl=np.nan\n            UMIfunc=np.nan\n        if umi and overseq_threshold is None:\n            reads_per_umi = round(Rclc/UMIcl, 2)\n\n        results.append([sample_id, chain, Rt, Ru_pc, Ra_pc, Roa_pc, UMIa, UMIc, overseq_threshold, Rf, UMIf, reads_per_umi, Ct, Rcl, Ctc, Rclc, Cfunc, Rfunc, UMIcl, UMIfunc])\n    result_df = pd.DataFrame(results, columns=[\"sample_id\", \"extracted_chain\", \"reads_total\", \"reads_with_umi_pc\", \"reads_aligned_pc\", \"reads_overlapped_aln_pc\",\n                                               \"total_umi\", \"umi_after_correction\", \"overseq_threshold\", \"reads_after_filter\", \"umi_after_filter\",\n                                               \"reads_per_umi\", \"clones_total\", \"reads_in_clones_total\", \"clones\", \"reads_in_clones\", \"clones_func\", \"reads_in_func_clones\", \"umi_in_clones\", \"umi_in_func_clones\"])\n    if not show_offtarget:\n        result_df = result_df.loc[result_df.reads_in_clones/result_df.reads_in_clones_total &gt; off_target_chain_threshold]\n    return result_df.sort_values(by=\"sample_id\").reset_index(drop=True)\n</code></pre>"},{"location":"functions/#mixcr.mixcr4_analyze_batch","title":"<code>mixcr4_analyze_batch(sample_df, output_folder, command_template=None, mixcr_path='mixcr', memory=32, time_estimate=1.5, custom_tag_pattern_column=None)</code>","text":"<p>Function for batch runs of MiXCR software using SLURM. For each record in the given <code>sample_df</code> this function creates a SLURM-script in <code>~/temp/slurm</code> folder and adds them to SLURM-queue. All the <code>stdout</code> logs are also  put to <code>~/temp/slurm</code> folder. In case of troubles check the latest logs in this folder.  By default this function uses <code>mixcr analyze</code> command for MiLab Hum RNA TCR Kit (with UMI).  To change the command template use <code>command_template</code> parameter</p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame, containing 'sample_id' column and  'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files</p> required <code>output_folder</code> <code>str</code> <p>path to output folder</p> required <code>command_template</code> <code>str</code> <p>MiXCR command template  (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix'). May be used as an example. Note that <code>mixcr analyze</code> and <code>r1 r2 output_prefix</code> are  \"magical\" parts of the template that should be kept as-is in the template, so change  only the part in-between these parts.</p> <code>None</code> <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>required OOM in GB</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>time estimate in hours for the calculation. It is the limit for SLURM task</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_analyze_batch(sample_df, output_folder, command_template=None,\n                         mixcr_path=\"mixcr\", memory=32, time_estimate=1.5, custom_tag_pattern_column=None):\n\n    \"\"\"\n    Function for batch runs of MiXCR software using SLURM.\n    For each record in the given `sample_df` this function creates a SLURM-script in\n    `~/temp/slurm` folder and adds them to SLURM-queue. All the `stdout` logs are also \n    put to `~/temp/slurm` folder. In case of troubles check the latest logs in this folder. \n    By default this function uses `mixcr analyze` command for MiLab Hum RNA TCR Kit (with UMI). \n    To change the command template use `command_template` parameter\n\n    Args:\n        sample_df (pd.DataFrame): DataFrame, containing 'sample_id' column and \n            'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files\n        output_folder (str): path to output folder\n        command_template (str): MiXCR command template \n            (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix').\n            May be used as an example. Note that `mixcr analyze` and `r1 r2 output_prefix` are \n            \"magical\" parts of the template that should be kept as-is in the template, so change \n            only the part in-between these parts.\n        mixcr_path (str): path to MiXCR binary\n        memory (int): required OOM in GB\n        time_estimate (numeric): time estimate in hours for the calculation. It\n            is the limit for SLURM task\n\n    Returns:\n        None\n    \"\"\"\n    max_memory = 1500\n    min_memory = 16\n\n    program_name=\"MIXCR4 Analyze Batch\"\n    samples_num = sample_df.shape[0]\n\n    # by default use the most popular preset for MiLaboratory Human TCR UMI MULTIPLEX Kit\n    default_command_template = \"mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix\"\n    if command_template is None:\n        command_template = default_command_template\n\n    # cut placeholders from command template\n    remove_list = [\"mixcr\", \"r1\", \"r2\", \"output_prefix\"]\n    command_template = ' '.join([w for w in command_template.split() if w not in remove_list])\n\n    # check input for custom tag pattern\n    custom_tag_pattern = False\n    if isinstance(custom_tag_pattern_column, str):\n        if custom_tag_pattern_column not in sample_df.columns:\n            raise ValueError(f\"Specified tag-pattern columns '{custom_tag_pattern_column}' is not present in sample_df\")\n        if \"--tag-pattern\" in command_template.split():\n            raise ValueError(f\"Please, remove '--tag-pattern' option from command_template, when you use custom tag-pattern\")\n        custom_tag_pattern = True\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    # time_estimate=1.5\n    cpus=40\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n    #   output_prefix = os.path.join(output_folder, sample_id)\n        output_prefix = sample_id\n        if custom_tag_pattern:\n            tag_pattern = r[custom_tag_pattern_column]\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} --tag-pattern \"{tag_pattern}\" {r1} {r2} {output_prefix}'\n        else:\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} {r1} {r2} {output_prefix}'\n        command = f\"cd {output_folder}; \" + command\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"functions/#mixcr.mixcr4_reports","title":"<code>mixcr4_reports(folder, mixcr_path='mixcr')</code>","text":"<p>runs <code>mixcr exportQc</code> commands - <code>align</code>, <code>chainUsage</code> and <code>tags</code> in a given folder  for all <code>.clns</code> filenames. <code>align</code> and <code>chainUsage</code> are run twice to create both  <code>svg</code> and <code>pdf</code> files.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to run the <code>mixcr exportQc</code> commands</p> required <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <p>Returns:     None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_reports(folder, mixcr_path=\"mixcr\"):\n\n    \"\"\"\n    runs `mixcr exportQc` commands - `align`, `chainUsage` and `tags` in a given folder \n    for all `.clns` filenames. `align` and `chainUsage` are run twice to create both \n    `svg` and `pdf` files.\n\n    Args:\n        folder (str): folder in which to run the `mixcr exportQc` commands\n        mixcr_path (str): path to MiXCR binary\n    Returns:\n        None\n\n    \"\"\"\n\n\n    program_name=\"MIXCR4.3 Reports\"\n    time_estimate=1\n    cpus=40\n    memory=32\n\n    # clns_filenames = os.path.join(folder, \"*.clns\")\n    # align_filename = os.path.join(folder, \"alignQc.png\")\n    # chains_filename = os.path.join(folder, \"chainsQc.png\")\n    # tags_filename = os.path.join(folder, \"tagsQc.pdf\")\n    clns_filenames = \"*.clns\"\n    align_filename = \"alignQc.svg\"\n    chains_filename = \"chainsQc.svg\"\n    align_filename_pdf = \"alignQc.pdf\"\n    chains_filename_pdf = \"chainsQc.pdf\"\n    tags_filename = \"tagsQc.pdf\"\n    #tables_filename = os.path.join(folder, \"tables.tsv\")\n    #preproc_filename = os.path.join(folder, \"preproc_tables.tsv\")\n    #postanalysis_filename = os.path.join(folder, \"postanalysis.json\")\n\n\n\n    commands = {\"alignQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename}\",\n                \"chainUsage\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename}\",\n                \"alignQcPDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename_pdf}\",\n                \"chainUsagePDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename_pdf}\",\n                \"tagsQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc tags -f {clns_filenames} {tags_filename}\"#,\n                #\"postanalysis\": f\"{MIXCR} -Xmx32g postanalysis individual -f --default-downsampling none --default-weight-function umi --only-productive --tables {tables_filename} --preproc-tables {preproc_filename} {clns_filenames} {postanalysis_filename}\"\n               }\n\n\n    commands_num = len(commands)\n\n    slurm_batch_filename = os.path.join(folder, \"mixcr_reports_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, commands_num)\n\n    for jobname, command in commands.items():\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n</code></pre>"},{"location":"functions/#mixcr.mixcr_7genes_run_batch","title":"<code>mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path='mixcr', memory=32, time_estimate=1.5)</code>","text":"<p>Function for batch runs of the MiXCR software using the SLURM <code>mixcr analyze</code> command and the <code>Human 7GENES DNA Multiplex</code> MiXCR built-in preset.  Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous  step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement. For each record in the given <code>sample_df</code>, this function creates a SLURM script in the <code>~/temp/slurm</code> folder and adds it to the SLURM queue.  All <code>stdout</code> logs are also saved to the <code>~/temp/slurm</code> folder. In case of troubles, check the latest logs in this folder. </p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame containing a 'sample_id' column and  'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.</p> required <code>output_folder</code> <code>str</code> <p>Path to the output folder.</p> required <code>mixcr_path</code> <code>str</code> <p>Path to the MiXCR binary.</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>Required OOM in GB.</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>Time estimate in hours for the calculation; it  is the limit for the SLURM task.</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path=\"mixcr\", memory=32, time_estimate=1.5):\n    \"\"\"\n    Function for batch runs of the MiXCR software using the SLURM `mixcr analyze` command and the `Human 7GENES DNA Multiplex` MiXCR built-in preset. \n    Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous \n    step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement.\n    For each record in the given `sample_df`, this function creates a SLURM script in the `~/temp/slurm` folder and adds it to the SLURM queue. \n    All `stdout` logs are also saved to the `~/temp/slurm` folder. In case of troubles, check the latest logs in this folder. \n\n    Args:\n        sample_df (pd.DataFrame): DataFrame containing a 'sample_id' column and \n            'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.\n        output_folder (str): Path to the output folder.\n        mixcr_path (str): Path to the MiXCR binary.\n        memory (int): Required OOM in GB.\n        time_estimate (numeric): Time estimate in hours for the calculation; it \n            is the limit for the SLURM task.\n\n    Returns:\n        None\n    \"\"\"\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    max_memory = 1500\n    min_memory = 16\n    cpus=40\n\n    program_name=\"MIXCR4 Analyze 7genes Batch\"\n    samples_num = sample_df.shape[0]\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    list_of_incomplete_rearrangements = [\"DJ_TRB\", \"VDD_TRD\", \"DDJ_TRD\", \"DD_TRD\", \"DJ_IGH\", \"VKDE_IGK\", \"CINTRON_KDE_IGK\"]\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n        output_prefix = sample_id\n\n        R1na = f\"{sample_id}_R1_not_aligned.fastq.gz\"\n        R2na = f\"{sample_id}_R2_not_aligned.fastq.gz\"\n\n        commands = [f\"cd {output_folder}\"]\n\n        first_command = f'{mixcr_path} -Xmx{memory}g analyze milab-human-dna-xcr-7genes-multiplex -f --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n        commands.append(first_command)\n\n        for rearrangement in list_of_incomplete_rearrangements:\n\n            # swap r and Rna so we would not implement copy of R_na\n            r1, R1na = R1na, r1\n            r2, R2na = R2na, r2\n\n            output_prefix = os.path.join(rearrangement, sample_id)\n\n            R1na = f\"{output_prefix}_R1_not_aligned.fastq.gz\"\n            R2na = f\"{output_prefix}_R2_not_aligned.fastq.gz\"\n\n            i_r_command = f'{mixcr_path} -Xmx{memory}g analyze generic-amplicon -f --species hs --library {rearrangement} --dna --floating-left-alignment-boundary --floating-right-alignment-boundary J -MexportClones.splitFilesBy=[] --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n            commands.append(i_r_command)\n\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        commands.append(f'echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}')\n\n        # join commands by &amp;&amp; so that next command runs if previous was finished without error and add new lines to the script\n        command = \" &amp;&amp; \\\\ \\n\".join(commands)\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n        # print(command)\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"functions/#mixcr.show_report_images","title":"<code>show_report_images(folder)</code>","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"functions/#pgen_calculation","title":"pgen_calculation","text":"<p>calculates probability of clonotype generation using OLGA model</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>An output of read_clonoset() function </p> required <code>overlap_type</code> <code>str</code> <p>Possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaVJ'</code> <code>mismatches</code> <code>int</code> <p>number of mismatches in heighbors CDR3 sequence, default=1</p> <code>1</code> <code>generation_model</code> <code>str</code> <p>generation model used by OLGA. Possible values are <code>human_B_heavy</code>, <code>human_B_kappa</code>,  <code>human_B_lambda</code>, <code>human_T_alpha</code>, <code>human_T_beta</code>, <code>mouse_T_alpha</code>, <code>mouse_T_beta</code>, default='human_T_beta'</p> <code>'human_T_beta'</code> <p>Returns:</p> Name Type Description <code>clonosets_df</code> <code>DataFrame</code> <p>a dataframe with calculated p_gens for each clonotype</p> Source code in <code>repseq/pgen_calculation.py</code> <pre><code>def calculate_clonotypes_pgen(clonosets_df, cl_filter=None, overlap_type='aaVJ', mismatches=1, generation_model='human_T_beta', olga_warnings=False):\n\n    '''\n    calculates probability of clonotype generation using OLGA model\n\n    Args:\n        clonosets_df (pd.DataFrame): An output of read_clonoset() function \n        overlap_type (str): Possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): number of mismatches in heighbors CDR3 sequence, default=1\n        generation_model (str): generation model used by OLGA. Possible values are `human_B_heavy`, `human_B_kappa`, \n            `human_B_lambda`, `human_T_alpha`, `human_T_beta`, `mouse_T_alpha`, `mouse_T_beta`, default='human_T_beta'\n\n\n    Returns: \n        clonosets_df (pd.DataFrame): a dataframe with calculated p_gens for each clonotype\n    '''\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    pgen_model = create_olga_model(generation_model)\n\n    if not cl_filter:\n        cl_filter = clf.Filter(functionality=\"a\")\n    clns_olga = cl_filter.apply(clonosets_df)\n\n    # creating tuples of size 1-3 each according to the overlap_type \n    overlap_type_vars = []\n    if aa:\n        overlap_type_vars.append('cdr3aa')\n    else:\n        overlap_type_vars.append('cdr3nt')\n    if check_v:\n        overlap_type_vars.append('v')\n    if check_j:\n        overlap_type_vars.append('j')\n\n    # creating neighbours, a {(cdr3, v if required, j if required): [neighbours]} dict, and a {neighbour: [(cdr3, v if required, j if required)]} dict\n    clonoset_seqs = list(clns_olga[overlap_type_vars].itertuples(index=False, name=None))\n    clonoset_seqs_for_calculation = []\n    if len(overlap_type_vars) != 3:\n        for cln in clonoset_seqs:\n            if len(cln) == 2:\n                seq, segment = cln\n                v, j = segment, segment\n            elif len(cln) == 1:\n                seq = cln[0]\n            if not check_v:\n                v = None\n            if not check_j:\n                j = None\n            clonoset_seqs_for_calculation.append((seq, v, j))\n    else:\n        clonoset_seqs_for_calculation = clonoset_seqs\n\n    # create a temporary column to merge p_gen values with respective clonotypes\n    clonoset_total_pgen = clns_olga.copy()\n    clonoset_total_pgen['temp'] = clonoset_seqs\n    neighbours, cln_neighbours_dict = create_neighbours(clonoset_seqs, mismatches, check_v, check_j)\n\n    clonotype_pgen_dict = {}\n    neighbours_pgen_dict = {}\n\n    n_chunks = 40\n    chunk_size = len(clonoset_seqs_for_calculation) // n_chunks + 1\n    tasks_clns = []\n    for i in range(0, len(clonoset_seqs_for_calculation), chunk_size):\n        if i + chunk_size &lt; len(clonoset_seqs_for_calculation):\n            task = (clonoset_seqs_for_calculation[i:i + chunk_size], aa, pgen_model, olga_warnings)\n        else:\n            task = (clonoset_seqs_for_calculation[i:], aa, pgen_model, olga_warnings)\n        tasks_clns.append(task)\n\n    clonotype_pgen = run_parallel_calculation(calculate_pgen_mp, \n                                              tasks_clns, \n                                              'Calculating p_gen using OLGA', \n                                              'chunks_clonoset')\n    for el in clonotype_pgen:\n        clonotype_pgen_dict.update(el)\n    if len(overlap_type_vars) != 3:\n        clonotype_pgen_dict = {tuple([x for x in ct if x is not None]):pgen for ct, pgen in clonotype_pgen_dict.items()}\n\n    n_chunks = 40\n    chunk_size = len(neighbours) // n_chunks + 1\n    tasks_nb = []\n    for i in range(0, len(neighbours), chunk_size):\n        if i + chunk_size &lt; len(neighbours):\n            task = (neighbours[i:i + chunk_size], aa, pgen_model, olga_warnings)\n        else:\n            task = (neighbours[i:], aa, pgen_model, olga_warnings)\n        tasks_nb.append(task)\n\n    neighbours_pgen = run_parallel_calculation(calculate_pgen_mp, \n                                                tasks_nb, \n                                                'Calculating p_gen using OLGA', \n                                                'chunks_neighbors')\n    for el in neighbours_pgen:\n        neighbours_pgen_dict.update(el)\n\n    # matching (neighbor, pgen) pairs with their respective clonotypes\n    clonotype_neighbours_pgen = {ct: [(n, neighbours_pgen_dict[n]) for n in ns] \n                                 for ct, ns in cln_neighbours_dict.items()}\n\n    # pgen = sum(neighbors_pgen) - (C(len(cdr3),mismatches) - 1) * pgen(clonotype)\n    clonoset_total_pgen_to_add = {'temp': [], 'p_gen': []}\n    for ct, ns in clonotype_neighbours_pgen.items():\n        ns_pgen_sum = sum([pgen[1] for pgen in ns])\n        n_overlaps = comb(len(ct[0]), mismatches) - 1\n        pgen_ct = clonotype_pgen_dict[ct]\n        clonoset_total_pgen_to_add['temp'].append(ct)\n        clonoset_total_pgen_to_add['p_gen'].append(ns_pgen_sum - n_overlaps * pgen_ct)\n\n    clonoset_total_pgen_to_add = pd.DataFrame(clonoset_total_pgen_to_add)\n\n    result_df = pd.merge(clonoset_total_pgen_to_add, clonoset_total_pgen, how='right', on=['temp'])\n    result_df.drop('temp', axis=1, inplace=True)\n    pgen = result_df.pop('p_gen')\n    result_df['pgen'] = pgen\n    return result_df\n</code></pre>"},{"location":"functions/#processing_stats","title":"processing_stats","text":""},{"location":"functions/#segment_usage","title":"segment_usage","text":""},{"location":"functions/#slurm","title":"slurm","text":""},{"location":"functions/#stats","title":"stats","text":""},{"location":"functions/#stats.calc_clonoset_stats","title":"<code>calc_clonoset_stats(clonosets_df, cl_filter=None, verbose=True)</code>","text":"<p>Calculates statistics for clonosets regarding clonotype, read and UMI counts. Also gives counts for functional clonotypes and non-singletons: clonotypes,  having only one count (UMI count if present, read count in other cases). Clonosets are given to the function in a form of pd.DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: dataframe with clonotype statistics for each sample in clonosets_df</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_clonoset_stats(clonosets_df, cl_filter=None, verbose=True):\n    \"\"\"\n    Calculates statistics for clonosets regarding clonotype, read and UMI counts.\n    Also gives counts for functional clonotypes and non-singletons: clonotypes, \n    having only one count (UMI count if present, read count in other cases).\n    Clonosets are given to the function in a form of pd.DataFrame\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        cl_filter (Filter): clonoset filter - object from `clone_filter.py` module.\n\n    Returns:\n        pd.DataFrame: dataframe with clonotype statistics for each sample in clonosets_df\n    \"\"\"\n\n    df = generic_calculation(clonosets_df, calculate_clonoset_stats_cl, clonoset_filter=cl_filter, program_name=\"CalcClonosetStats\", verbose=verbose)\n    convert_dict = {\"clones\": int,\n                    \"clones_func\": int,\n                    \"clones_func_singletons\": int,\n                    \"clones_func_non_singletons\": int,\n                    \"clones_nonfunc\": int,\n                    \"reads\": int,\n                    \"reads_func\": int,\n                    \"reads_nonfunc\": int}\n    if not df[\"umi\"].isnull().values.any():\n        convert_dict.update({\"umi\": int,\n                        \"umi_func\": int,\n                        \"umi_nonfunc\": int})\n\n    df = df.astype(convert_dict)\n    return df\n</code></pre>"},{"location":"functions/#stats.calc_diversity_stats","title":"<code>calc_diversity_stats(clonosets_df, cl_filter=None, iterations=3, seed=None, drop_small_samples=True)</code>","text":"<p>Calculates <code>observed diversity</code>, <code>Shannon-Wiener</code> and <code>normalized Shannon-Wiener</code> index for each clonoset in <code>clonosets_df</code>. <code>observed diversity</code> - total number of unique clonotypes in a given clonoset <code>Shannon-Wiener</code> - mixed evenness and diversity metric <code>normalized Shannon-Wiener</code> - evenness metric. It is highly recommnded to use equal downsampling for all input clonosets for </p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>segment</code> <code>str</code> <p>possible values are <code>v</code>, <code>j</code> or <code>c</code>. Defaults to \"v\".</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <code>table</code> <code>str</code> <p>table type - <code>long</code> or <code>wide</code>. Defaults to \"long\".</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in the function description. If 'wide' - then it has all possible segments in  column names, sample_id's - in rows and usage in each cell in the table.</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_diversity_stats(clonosets_df, cl_filter=None, iterations=3, seed=None, drop_small_samples=True):\n    \"\"\"\n    Calculates `observed diversity`, `Shannon-Wiener` and `normalized Shannon-Wiener` index for\n    each clonoset in `clonosets_df`.\n    `observed diversity` - total number of unique clonotypes in a given clonoset\n    `Shannon-Wiener` - mixed evenness and diversity metric\n    `normalized Shannon-Wiener` - evenness metric.\n    It is highly recommnded to use equal downsampling for all input clonosets for \n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        segment (str, optional): possible values are `v`, `j` or `c`. Defaults to \"v\".\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        table (str, optional): table type - `long` or `wide`. Defaults to \"long\".\n\n\n    Returns:\n        pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in\n            the function description. If 'wide' - then it has all possible segments in \n            column names, sample_id's - in rows and usage in each cell in the table.\n    \"\"\"\n    df = generic_calculation(clonosets_df, calculate_diversity_stats_cl, clonoset_filter=cl_filter,\n                             program_name=\"CalcDiversityStats\", iterations=iterations, seed=seed, drop_small_samples=drop_small_samples)\n    return df\n</code></pre>"},{"location":"functions/#stats.calc_segment_usage","title":"<code>calc_segment_usage(clonosets_df, segment='v', cl_filter=None, table='long', by_count=False)</code>","text":"<p>Calculates segment (<code>V</code>, <code>J</code>, or <code>C</code>) usage for several samples. By default outputs 'long' table with four columns: segment name, <code>usage</code>, <code>sample_id</code> and <code>chain</code>. It also may take a clone_filter as input: <code>cl_filter</code> from <code>clone_filter</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>segment</code> <code>str</code> <p>possible values are <code>v</code>, <code>j</code> or <code>c</code>. Defaults to \"v\".</p> <code>'v'</code> <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <code>table</code> <code>str</code> <p>table type - <code>long</code> or <code>wide</code>. Defaults to \"long\".</p> <code>'long'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in the function description. If 'wide' - then it has all possible segments in  column names, sample_id's - in rows and usage in each cell in the table.</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_segment_usage(clonosets_df, segment=\"v\", cl_filter=None, table=\"long\", by_count=False):\n    \"\"\"\n    Calculates segment (`V`, `J`, or `C`) usage for several samples. By default outputs\n    'long' table with four columns: segment name, `usage`, `sample_id` and `chain`.\n    It also may take a clone_filter as input: `cl_filter` from `clone_filter` module.\n\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        segment (str, optional): possible values are `v`, `j` or `c`. Defaults to \"v\".\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        table (str, optional): table type - `long` or `wide`. Defaults to \"long\".\n\n\n    Returns:\n        pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in\n            the function description. If 'wide' - then it has all possible segments in \n            column names, sample_id's - in rows and usage in each cell in the table.\n    \"\"\"\n\n\n    if cl_filter is None:\n        cl_filter = Filter()\n\n    table_options = [\"long\", \"wide\"]\n    if table not in table_options:\n        raise ValueError(f\"Unknown value for 'table' parameter. Possible options: {', '.join(table_options)}\")\n\n    possible_segments = [\"v\", \"j\", \"c\", \"vj\", \"vlen\", \"vjlen\"]\n    segment = segment.lower()\n    if segment not in possible_segments:\n        raise ValueError(f\"Wrong segment value. Possible values: {', '.join(possible_segments)}\")\n    df = generic_calculation(clonosets_df, calc_segment_usage_cl, clonoset_filter=cl_filter, program_name=\"CalcSegmentUsage\", segment=segment, by_count=by_count)\n    df = df.fillna(0)\n    if table == \"wide\":\n        return df\n    else:\n        return df.melt(id_vars=[\"sample_id\", \"chain\"]).rename(columns={\"value\":\"usage\", \"variable\":segment})\n</code></pre>"},{"location":"functions/#stats.generic_calculation","title":"<code>generic_calculation(clonosets_df_in, calc_function, clonoset_filter=None, program_name='Calculation', iterations=1, seed=None, drop_small_samples=False, verbose=True, skip_checks=False, **kwargs)</code>","text":"<p>Main function that applies batch calculations for multiple clonosets using a <code>calc_function</code>. It checks inputs, checks if clonotype counts are  coherent with downsample or top numbers in clonoset filter. All filters and calc_function are applied to each clonoset in parallel, if several cores are available. After all calculations are finished, this function combines the results into  one pd.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF. <code>sample_id</code>'s or <code>sample_id</code>+<code>chain</code> combinations must be unique in this DF.</p> required <code>calc_function</code> <code>function_name</code> <p>function, applicable to a single clonoset (pd.DataFrame) in VDJtools-like format that returns a dictionary of properties+values as output.</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> required <code>program_name</code> <code>str</code> <p>the name of applied calculation, it is shown in the progress-bar</p> <code>'Calculation'</code> <code>iterations</code> <code>int</code> <p>number of iterations to obtain mean values for calculations when random processes (downsampling/mix_tails) in clonoset filter are applied. Recommended to use 3-5 iterations.</p> <code>1</code> <code>seed</code> <code>hashable</code> <p>seed for random events (downsampling/mix_tails). It overrides the  values, specified in <code>cl_filter</code>.</p> <code>None</code> <code>drop_small_samples</code> <code>bool</code> <p><code>True</code> - samples, that can't be downsampled or top-cropped because of lack of counts/clonotypes will be dropped before the calculation. <code>False</code> - small samples will be taken into account, but with fewer counts/clonotypes  than those with enough counts/clonotypes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>resulting DataFrame, with <code>sample_id</code> and <code>chain</code> columns and properties columns for each clonoset.</p> Source code in <code>repseq/stats.py</code> <pre><code>def generic_calculation(clonosets_df_in, calc_function, clonoset_filter=None, program_name=\"Calculation\", iterations=1, seed=None, drop_small_samples=False, verbose=True, skip_checks=False, **kwargs):\n    '''\n    Main function that applies batch calculations for multiple clonosets\n    using a `calc_function`. It checks inputs, checks if clonotype counts are \n    coherent with downsample or top numbers in clonoset filter.\n    All filters and calc_function are applied to each clonoset in parallel, if\n    several cores are available.\n    After all calculations are finished, this function combines the results into \n    one pd.DataFrame.\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n            `sample_id`'s or `sample_id`+`chain` combinations must be unique in this DF.\n        calc_function (function_name): function, applicable to a single clonoset (pd.DataFrame)\n            in VDJtools-like format that returns a dictionary of properties+values as output.\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        program_name (str): the name of applied calculation, it is shown in the progress-bar\n        iterations (int): number of iterations to obtain mean values for calculations when\n            random processes (downsampling/mix_tails) in clonoset filter are applied.\n            Recommended to use 3-5 iterations.\n        seed (hashable): seed for random events (downsampling/mix_tails). It overrides the \n            values, specified in `cl_filter`.\n        drop_small_samples (bool): `True` - samples, that can't be downsampled or top-cropped\n            because of lack of counts/clonotypes will be dropped before the calculation.\n            `False` - small samples will be taken into account, but with fewer counts/clonotypes \n            than those with enough counts/clonotypes.\n\n    Returns:\n        df (pd.DataFrame): resulting DataFrame, with `sample_id` and `chain` columns and properties\n            columns for each clonoset.\n\n    '''\n\n\n    columns_retain = [\"sample_id\"]\n    clonosets_df = clonosets_df_in.copy()\n\n    if \"sample_id\" not in clonosets_df.columns:\n        raise ValueError(\"Clonoset_df does not contain required column 'sample_id'\")\n    if \"filename\" not in clonosets_df.columns:\n        raise ValueError(\"Clonoset_df does not contain required column 'filename'\")\n\n    split_chain_after_calculation = False\n    if \"chain\" not in clonosets_df.columns:\n        if len(clonosets_df) != len(clonosets_df.sample_id.unique()):\n            raise ValueError(\"Clonoset_df contains nonunique sample_ids\")\n    else:\n        columns_retain.append(\"chain\")\n        if len(clonosets_df[[\"sample_id\", \"chain\"]].drop_duplicates()) != len(clonosets_df):\n            raise ValueError(\"Clonoset_df contains nonunique sample_id+chain combinations\")\n        if len(clonosets_df) != len(clonosets_df.sample_id.unique()):\n            clonosets_df[\"sample_id\"] = clonosets_df[\"sample_id\"] + \"_\" + clonosets_df[\"chain\"]\n            split_chain_after_calculation = True\n\n    random_filter = False\n    need_downsample = False\n    need_top = False\n\n    count_column_by_umi_and_functionality = {\n            True: {\"a\": \"umi\",\n                   \"f\": \"umi_func\",\n                   \"n\": \"umi_nonfunc\"},\n            False: {\"a\": \"reads\",\n                    \"f\": \"reads_func\",\n                    \"n\": \"reads_nonfunc\"}\n        }\n\n    clone_column_by_functionality = {\n        \"a\": \"clones\",\n        \"f\": \"clones_func\",\n        \"n\": \"clones_nonfunc\"\n    }\n\n    exclude_samples = set()\n\n\n    if clonoset_filter is not None and not skip_checks:\n        if isinstance(clonoset_filter.downsample_size, int):\n            need_downsample = True\n        if isinstance(clonoset_filter.top, int):\n            need_top = True\n        if need_downsample or need_top:\n            print(\"Calcultating stats for original clonosets\\n\" + \"_\"*41)\n            stats = calc_clonoset_stats(clonosets_df, verbose=verbose)\n            downsample_column = count_column_by_umi_and_functionality[clonoset_filter.by_umi][clonoset_filter.functionality]\n            read_column = count_column_by_umi_and_functionality[False][clonoset_filter.functionality]\n            top_column = clone_column_by_functionality[clonoset_filter.functionality]\n\n        if need_downsample:\n            count_by_reads_samples = set()\n            if stats[downsample_column].isnull().any().any():\n                nan_downsample_samples = list(stats[stats[downsample_column].isna()].sample_id)\n                print(f\"WARNING! Following samples have NaN downsample counts ('{downsample_column}'): {', '.join(nan_downsample_samples)}\")\n                if clonoset_filter.by_umi:\n                    print(\"These samples will be counted by reads instead\")\n                    count_by_reads_samples = set(nan_downsample_samples)\n                else:\n                    print(\"These samples will be excluded from further calculations.\")\n                    exclude_samples.update(nan_downsample_samples)\n            not_enough_count_df = stats[(stats[downsample_column] &lt; clonoset_filter.downsample_size) &amp; (~stats.sample_id.isin(count_by_reads_samples)) |\n                                        (stats[read_column] &lt; clonoset_filter.downsample_size) &amp; (stats.sample_id.isin(count_by_reads_samples))]\n            if len(not_enough_count_df) &gt; 0:\n                not_enough_count_samples = list(not_enough_count_df.sample_id)\n                if not drop_small_samples:\n                    print(f\"WARNING! Following samples have not enough downsample counts ('{downsample_column}' &lt; {clonoset_filter.downsample_size}): {', '.join(not_enough_count_samples)}\")\n                    print(\"These samples will be excluded from further calculations.\")\n                    print(\"To suppress the warnings set drop_small_samples=True\")\n                exclude_samples.update(not_enough_count_samples)\n        if need_top:\n            if stats[downsample_column].isnull().any().any():\n                nan_downsample_samples = list(stats[stats[downsample_column].isna()].sample_id)\n                print(f\"WARNING! Following samples have NaN counts ('{downsample_column}'): {', '.join(nan_downsample_samples)}\")\n                if clonoset_filter.by_umi:\n                    print(\"These samples will be counted by reads instead\")\n                else:\n                    print(\"These samples will be excluded from further calculations.\")\n                    exclude_samples.update(nan_downsample_samples)\n            not_enough_clones_df = stats[stats[top_column] &lt; clonoset_filter.top]\n            if len(not_enough_clones_df) &gt; 0:\n                not_enough_clones_samples = list(not_enough_clones_df.sample_id)\n                if not drop_small_samples:\n                    print(f\"WARNING! Following samples have not enough clonotypes ('{top_column}' &lt; {clonoset_filter.top}): {', '.join(not_enough_clones_samples)}\")\n                    print(\"These samples will be excluded from further calculations.\")\n                    print(\"To suppress the warnings set drop_small_samples=True\")\n                exclude_samples.update(not_enough_clones_samples)\n\n\n        if isinstance(clonoset_filter.downsample_size, int) or (isinstance(clonoset_filter.top, int) and clonoset_filter.mix_tails):\n            random_filter = True\n\n\n    if random_filter and seed is None:\n        print(\"WARNING! Random filter is applied, but random seed is not set. This may lead to non-reproducible results.\")\n        print(\"You may set the seed (of any hashable type) by specifying 'seed='\")\n\n    tasks = []\n    for i,r in clonosets_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        filename = r[\"filename\"]\n        if sample_id in exclude_samples:\n            continue\n        if clonoset_filter is not None:\n            task = (sample_id, filename, calc_function, clonoset_filter.spawn(), iterations, seed, program_name, random_filter, kwargs)\n        else:\n            task = (sample_id, filename, calc_function, None, iterations, seed, program_name, random_filter, kwargs)\n        tasks.append(task)\n\n    results = run_parallel_calculation(perform_generic_calculation_mp, tasks, program_name, object_name=\"calcultaion(s)\", verbose=verbose)\n    clonosets_df = clonosets_df[columns_retain]\n    df = clonosets_df.merge(pd.DataFrame(results), how=\"left\")\n    if split_chain_after_calculation:\n        df[\"sample_id\"] = df[\"sample_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    return df\n</code></pre>"},{"location":"functions/#stats.perform_generic_calculation_mp","title":"<code>perform_generic_calculation_mp(args)</code>","text":"<p>A single-core \"worker\" for <code>generic_calculation</code> function. It applies clonoset filter (several times if <code>iterations</code> &gt; 1) and performes a calculation for a single filtered clonoset.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>tuple, containing all required parameters</p> required <p>Returns:</p> Name Type Description <code>clonoset_result</code> <code>dict</code> <p>a dictionary of parameters, calculated for given clonoset and averaged for iterations (if &gt; 1)</p> Source code in <code>repseq/stats.py</code> <pre><code>def perform_generic_calculation_mp(args):\n    '''\n    A single-core \"worker\" for `generic_calculation` function.\n    It applies clonoset filter (several times if `iterations` &gt; 1) and performes a\n    calculation for a single filtered clonoset.\n\n\n    Args:\n        args (tuple): tuple, containing all required parameters\n\n    Returns:\n        clonoset_result (dict): a dictionary of parameters, calculated for given clonoset\n            and averaged for iterations (if &gt; 1)\n    '''\n\n\n    (sample_id, filename, calc_function, clonoset_filter, iterations, seed, program_name, random_filter, kwargs) = args\n    clonoset = read_clonoset(filename)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    if random_filter and isinstance(seed, int):\n        random.seed(seed)\n\n    clonoset_result = {\"sample_id\": sample_id}\n    clonoset_results = []\n    filtered_clonosets = []\n\n    for i in range(iterations):\n        if clonoset_filter is not None:\n            filtered_clonoset = clonoset_filter.apply(clonoset, colnames=colnames)\n        else:\n            filtered_clonoset = clonoset\n        filtered_clonosets.append(filtered_clonoset)\n\n    for filtered_clonoset in filtered_clonosets:\n        clonoset_results.append(calc_function(filtered_clonoset, **kwargs))\n    clonoset_result.update(pd.DataFrame(clonoset_results).mean().to_dict())\n\n    return clonoset_result\n</code></pre>"},{"location":"functions/#tcrdist_clustering","title":"tcrdist_clustering","text":""},{"location":"functions/#tcrdist_clusters_slurm","title":"tcrdist_clusters_slurm","text":""},{"location":"functions/#vdjtools","title":"vdjtools","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#library-installation","title":"Library installation","text":"<ul> <li>Clone the library into a designated directory: <code>git clone https://github.com/mmjmike/repseq</code></li> <li>To update the library use <code>git pull -p</code> inside its folder</li> </ul> <p>For instance, if <code>~/soft</code> directory is used:     <code>mkdir ~/soft</code> <code>cd ~/soft</code> <code>git clone https://github.com/mmjmike/repseq</code> <code>cd ~/soft/repseq</code> <code>git pull -p</code></p>"},{"location":"installation/#setting-up-the-environment","title":"Setting up the environment","text":"<p>To resolve all dependencies, you need to set up a Conda environment.</p> <ul> <li>Download the file main_repseq.yml, for example, into the folder <code>~/resources/conda_envs</code></li> <li>Enter that folder: <code>cd ~/resources/conda_envs</code></li> <li>Make sure Conda is working: You should see (base) or another environment name on the left side of the command prompt. If it\u2019s not active, run: <code>conda activate bash</code></li> <li>Create the environment and install dependencies from the .yml file: <code>conda env create -n main_repseq -f main_repseq.yml</code> (-n is used to specify the environment name)</li> <li>Wait for all dependencies to finish installing. Shortly after that, the new environment will appear in the environment list in Jupyter Hub.</li> </ul> <p>In Jupyter Hub, make sure to select the installed environment from the menu in the upper-right corner. For clustering and calculating statistics, it's recommended to run the Jupyter Hub server in a short session with 32 processors to fully utilize parallel computations during clustering.</p>"},{"location":"mixcr/","title":"MiXCR4 functions for batch analysis in Jupyter. Uses SLURM on Aldan3 server","text":""},{"location":"mixcr/#mixcr4_analyze_batch","title":"mixcr4_analyze_batch","text":"<p>Function for batch runs of MiXCR software using SLURM. For each record in the given <code>sample_df</code> this function creates a SLURM-script in <code>~/temp/slurm</code> folder and adds them to SLURM-queue. All the <code>stdout</code> logs are also  put to <code>~/temp/slurm</code> folder. In case of troubles check the latest logs in this folder.  By default this function uses <code>mixcr analyze</code> command for MiLab Hum RNA TCR Kit (with UMI).  To change the command template use <code>command_template</code> parameter</p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame, containing 'sample_id' column and  'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files</p> required <code>output_folder</code> <code>str</code> <p>path to output folder</p> required <code>command_template</code> <code>str</code> <p>MiXCR command template  (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix'). May be used as an example. Note that <code>mixcr analyze</code> and <code>r1 r2 output_prefix</code> are  \"magical\" parts of the template that should be kept as-is in the template, so change  only the part in-between these parts.</p> <code>None</code> <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>required OOM in GB</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>time estimate in hours for the calculation. It is the limit for SLURM task</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_analyze_batch(sample_df, output_folder, command_template=None,\n                         mixcr_path=\"mixcr\", memory=32, time_estimate=1.5, custom_tag_pattern_column=None):\n\n    \"\"\"\n    Function for batch runs of MiXCR software using SLURM.\n    For each record in the given `sample_df` this function creates a SLURM-script in\n    `~/temp/slurm` folder and adds them to SLURM-queue. All the `stdout` logs are also \n    put to `~/temp/slurm` folder. In case of troubles check the latest logs in this folder. \n    By default this function uses `mixcr analyze` command for MiLab Hum RNA TCR Kit (with UMI). \n    To change the command template use `command_template` parameter\n\n    Args:\n        sample_df (pd.DataFrame): DataFrame, containing 'sample_id' column and \n            'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files\n        output_folder (str): path to output folder\n        command_template (str): MiXCR command template \n            (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix').\n            May be used as an example. Note that `mixcr analyze` and `r1 r2 output_prefix` are \n            \"magical\" parts of the template that should be kept as-is in the template, so change \n            only the part in-between these parts.\n        mixcr_path (str): path to MiXCR binary\n        memory (int): required OOM in GB\n        time_estimate (numeric): time estimate in hours for the calculation. It\n            is the limit for SLURM task\n\n    Returns:\n        None\n    \"\"\"\n    max_memory = 1500\n    min_memory = 16\n\n    program_name=\"MIXCR4 Analyze Batch\"\n    samples_num = sample_df.shape[0]\n\n    # by default use the most popular preset for MiLaboratory Human TCR UMI MULTIPLEX Kit\n    default_command_template = \"mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix\"\n    if command_template is None:\n        command_template = default_command_template\n\n    # cut placeholders from command template\n    remove_list = [\"mixcr\", \"r1\", \"r2\", \"output_prefix\"]\n    command_template = ' '.join([w for w in command_template.split() if w not in remove_list])\n\n    # check input for custom tag pattern\n    custom_tag_pattern = False\n    if isinstance(custom_tag_pattern_column, str):\n        if custom_tag_pattern_column not in sample_df.columns:\n            raise ValueError(f\"Specified tag-pattern columns '{custom_tag_pattern_column}' is not present in sample_df\")\n        if \"--tag-pattern\" in command_template.split():\n            raise ValueError(f\"Please, remove '--tag-pattern' option from command_template, when you use custom tag-pattern\")\n        custom_tag_pattern = True\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    # time_estimate=1.5\n    cpus=40\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n    #   output_prefix = os.path.join(output_folder, sample_id)\n        output_prefix = sample_id\n        if custom_tag_pattern:\n            tag_pattern = r[custom_tag_pattern_column]\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} --tag-pattern \"{tag_pattern}\" {r1} {r2} {output_prefix}'\n        else:\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} {r1} {r2} {output_prefix}'\n        command = f\"cd {output_folder}; \" + command\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"mixcr/#mixcr_7genes_run_batch","title":"mixcr_7genes_run_batch","text":"<p>Function for batch runs of the MiXCR software using the SLURM <code>mixcr analyze</code> command and the <code>Human 7GENES DNA Multiplex</code> MiXCR built-in preset.  Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous  step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement. For each record in the given <code>sample_df</code>, this function creates a SLURM script in the <code>~/temp/slurm</code> folder and adds it to the SLURM queue.  All <code>stdout</code> logs are also saved to the <code>~/temp/slurm</code> folder. In case of troubles, check the latest logs in this folder. </p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame containing a 'sample_id' column and  'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.</p> required <code>output_folder</code> <code>str</code> <p>Path to the output folder.</p> required <code>mixcr_path</code> <code>str</code> <p>Path to the MiXCR binary.</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>Required OOM in GB.</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>Time estimate in hours for the calculation; it  is the limit for the SLURM task.</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path=\"mixcr\", memory=32, time_estimate=1.5):\n    \"\"\"\n    Function for batch runs of the MiXCR software using the SLURM `mixcr analyze` command and the `Human 7GENES DNA Multiplex` MiXCR built-in preset. \n    Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous \n    step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement.\n    For each record in the given `sample_df`, this function creates a SLURM script in the `~/temp/slurm` folder and adds it to the SLURM queue. \n    All `stdout` logs are also saved to the `~/temp/slurm` folder. In case of troubles, check the latest logs in this folder. \n\n    Args:\n        sample_df (pd.DataFrame): DataFrame containing a 'sample_id' column and \n            'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.\n        output_folder (str): Path to the output folder.\n        mixcr_path (str): Path to the MiXCR binary.\n        memory (int): Required OOM in GB.\n        time_estimate (numeric): Time estimate in hours for the calculation; it \n            is the limit for the SLURM task.\n\n    Returns:\n        None\n    \"\"\"\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    max_memory = 1500\n    min_memory = 16\n    cpus=40\n\n    program_name=\"MIXCR4 Analyze 7genes Batch\"\n    samples_num = sample_df.shape[0]\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    list_of_incomplete_rearrangements = [\"DJ_TRB\", \"VDD_TRD\", \"DDJ_TRD\", \"DD_TRD\", \"DJ_IGH\", \"VKDE_IGK\", \"CINTRON_KDE_IGK\"]\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n        output_prefix = sample_id\n\n        R1na = f\"{sample_id}_R1_not_aligned.fastq.gz\"\n        R2na = f\"{sample_id}_R2_not_aligned.fastq.gz\"\n\n        commands = [f\"cd {output_folder}\"]\n\n        first_command = f'{mixcr_path} -Xmx{memory}g analyze milab-human-dna-xcr-7genes-multiplex -f --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n        commands.append(first_command)\n\n        for rearrangement in list_of_incomplete_rearrangements:\n\n            # swap r and Rna so we would not implement copy of R_na\n            r1, R1na = R1na, r1\n            r2, R2na = R2na, r2\n\n            output_prefix = os.path.join(rearrangement, sample_id)\n\n            R1na = f\"{output_prefix}_R1_not_aligned.fastq.gz\"\n            R2na = f\"{output_prefix}_R2_not_aligned.fastq.gz\"\n\n            i_r_command = f'{mixcr_path} -Xmx{memory}g analyze generic-amplicon -f --species hs --library {rearrangement} --dna --floating-left-alignment-boundary --floating-right-alignment-boundary J -MexportClones.splitFilesBy=[] --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n            commands.append(i_r_command)\n\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        commands.append(f'echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}')\n\n        # join commands by &amp;&amp; so that next command runs if previous was finished without error and add new lines to the script\n        command = \" &amp;&amp; \\\\ \\n\".join(commands)\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n        # print(command)\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"mixcr/#mixcr4_reports","title":"mixcr4_reports","text":"<p>runs <code>mixcr exportQc</code> commands - <code>align</code>, <code>chainUsage</code> and <code>tags</code> in a given folder  for all <code>.clns</code> filenames. <code>align</code> and <code>chainUsage</code> are run twice to create both  <code>svg</code> and <code>pdf</code> files.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to run the <code>mixcr exportQc</code> commands</p> required <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <p>Returns:     None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_reports(folder, mixcr_path=\"mixcr\"):\n\n    \"\"\"\n    runs `mixcr exportQc` commands - `align`, `chainUsage` and `tags` in a given folder \n    for all `.clns` filenames. `align` and `chainUsage` are run twice to create both \n    `svg` and `pdf` files.\n\n    Args:\n        folder (str): folder in which to run the `mixcr exportQc` commands\n        mixcr_path (str): path to MiXCR binary\n    Returns:\n        None\n\n    \"\"\"\n\n\n    program_name=\"MIXCR4.3 Reports\"\n    time_estimate=1\n    cpus=40\n    memory=32\n\n    # clns_filenames = os.path.join(folder, \"*.clns\")\n    # align_filename = os.path.join(folder, \"alignQc.png\")\n    # chains_filename = os.path.join(folder, \"chainsQc.png\")\n    # tags_filename = os.path.join(folder, \"tagsQc.pdf\")\n    clns_filenames = \"*.clns\"\n    align_filename = \"alignQc.svg\"\n    chains_filename = \"chainsQc.svg\"\n    align_filename_pdf = \"alignQc.pdf\"\n    chains_filename_pdf = \"chainsQc.pdf\"\n    tags_filename = \"tagsQc.pdf\"\n    #tables_filename = os.path.join(folder, \"tables.tsv\")\n    #preproc_filename = os.path.join(folder, \"preproc_tables.tsv\")\n    #postanalysis_filename = os.path.join(folder, \"postanalysis.json\")\n\n\n\n    commands = {\"alignQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename}\",\n                \"chainUsage\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename}\",\n                \"alignQcPDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename_pdf}\",\n                \"chainUsagePDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename_pdf}\",\n                \"tagsQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc tags -f {clns_filenames} {tags_filename}\"#,\n                #\"postanalysis\": f\"{MIXCR} -Xmx32g postanalysis individual -f --default-downsampling none --default-weight-function umi --only-productive --tables {tables_filename} --preproc-tables {preproc_filename} {clns_filenames} {postanalysis_filename}\"\n               }\n\n\n    commands_num = len(commands)\n\n    slurm_batch_filename = os.path.join(folder, \"mixcr_reports_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, commands_num)\n\n    for jobname, command in commands.items():\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n</code></pre>"},{"location":"mixcr/#get_processing_table","title":"get_processing_table","text":"<p>Searches for clonosets in the the folder, extracts their sample_id's and shows main processing stats in a table format. By default does not show \"off-target\" clonosets -  those having less than 1% (default, may be overriden) of reads for the sample_id. For example, you have sequenced TRB sample, but there is found 0.5% (by read count)  of TRA chains for the same sample_id, then the clonoset will not be shown in the table. You can specify <code>show_offtarget=True</code> to display all found chains in the table or  outherwise set a higher value for <code>off_target_chain_threshold</code> (<code>0.01</code> by default).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str or list</code> <p>folder or list of folders in which to look for clonosets and processing stats</p> required <code>show_offtarget</code> <code>bool</code> <p>add offtarget chains to the stats</p> <code>False</code> <code>off_target_chain_threshold</code> <code>float</code> <p>threshold for off-target chains</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe, containing <code>sample_id</code>, <code>extracted_chain</code> and  different processing stats columns. There may be several rows with the same  <code>sample_id</code>, with each found <code>extracted_chain</code></p> Source code in <code>repseq/mixcr.py</code> <pre><code>def get_processing_table(folder, show_offtarget=False, off_target_chain_threshold=0.01):\n    \"\"\"\n    Searches for clonosets in the the folder, extracts their sample_id's and shows main\n    processing stats in a table format. By default does not show \"off-target\" clonosets - \n    those having less than 1% (default, may be overriden) of reads for the sample_id.\n    For example, you have sequenced TRB sample, but there is found 0.5% (by read count) \n    of TRA chains for the same sample_id, then the clonoset will not be shown in the table.\n    You can specify `show_offtarget=True` to display all found chains in the table or \n    outherwise set a higher value for `off_target_chain_threshold` (`0.01` by default).\n\n    Args:\n        folder (str or list): folder or list of folders in which to look for clonosets and\n            processing stats\n        show_offtarget (bool): add offtarget chains to the stats\n        off_target_chain_threshold (float): threshold for off-target chains\n\n    Returns:\n        df (pd.DataFrame): dataframe, containing `sample_id`, `extracted_chain` and \n            different processing stats columns. There may be several rows with the same \n            `sample_id`, with each found `extracted_chain`\n    \"\"\"\n\n    if isinstance(folder, list):\n        tables = []\n        for f in folder:\n            table = get_processing_table(f, show_offtarget=show_offtarget)\n            tables.append(table)\n        return pd.concat(tables).sort_values(by=\"sample_id\").reset_index(drop=True)\n\n    results = []\n    clonosets = find_all_exported_clonosets_in_folder(folder, chain=None)\n\n    for i, r in clonosets.iterrows():\n        sample_id = r[\"sample_id\"]\n        chain = r[\"chain\"]\n        align_report = read_json_report(sample_id, folder, \"align\")\n\n        try:\n            refine_report = read_json_report(sample_id, folder, \"refine\")\n            umi = True\n        except FileNotFoundError:\n            umi = False\n\n        assemble_report = read_json_report(sample_id, folder, \"assemble\")\n\n        # print(sample_id, chain)\n        clonoset = read_clonoset(r.filename)\n        clonoset_f = filter_nonfunctional_clones(clonoset)\n\n        # align report\n        Rt=align_report[\"totalReadsProcessed\"]\n        Ru=align_report[\"totalReadsProcessed\"]-align_report[\"notAlignedReasons\"][\"NoBarcode\"]\n        Ru_pc = round(Ru/Rt*100, 2)\n        Ra=align_report[\"aligned\"]\n        Ra_pc = round(Ra/Rt*100, 2)\n        Roa = align_report[\"overlappedAligned\"]\n        Roa_pc = round(Roa/Ra*100, 2)\n\n        if umi:\n        #Ra2=refine_report[\"correctionReport\"][\"inputRecords\"] ##### differs from Ra, but D.Bolotin did not explain why\n\n            UMIa=refine_report[\"correctionReport\"][\"steps\"][0][\"inputDiversity\"]\n            UMIc=refine_report[\"correctionReport\"][\"steps\"][0][\"outputDiversity\"]\n            try:\n                UMIf=refine_report[\"correctionReport\"][\"filterReport\"][\"numberOfGroupsAccepted\"]\n            except TypeError:\n                UMIf=UMIc\n            Rf=refine_report[\"correctionReport\"][\"outputRecords\"]\n            try:\n                overseq_threshold = int(refine_report[\"correctionReport\"][\"filterReport\"][\"operatorReports\"][0][\"operatorReport\"][\"threshold\"])\n            except TypeError:\n                overseq_threshold = None\n            reads_per_umi = round(Rf/UMIf, 2)\n        else:\n            UMIa = np.nan\n            UMIc = np.nan\n            UMIf = np.nan\n            Rf = np.nan\n            overseq_threshold = np.nan\n            reads_per_umi = np.nan\n\n        Ct=assemble_report[\"clones\"]\n        Rcl=assemble_report[\"readsInClones\"]\n\n        Ctc=len(clonoset)\n        Rclc=int(clonoset.readCount.sum())\n\n        Cfunc=len(clonoset_f)\n        Rfunc=int(clonoset_f.readCount.sum())\n        if umi:\n            UMIcl=clonoset.uniqueMoleculeCount.sum()\n            UMIfunc=clonoset_f.uniqueMoleculeCount.sum()\n        else:\n            UMIcl=np.nan\n            UMIfunc=np.nan\n        if umi and overseq_threshold is None:\n            reads_per_umi = round(Rclc/UMIcl, 2)\n\n        results.append([sample_id, chain, Rt, Ru_pc, Ra_pc, Roa_pc, UMIa, UMIc, overseq_threshold, Rf, UMIf, reads_per_umi, Ct, Rcl, Ctc, Rclc, Cfunc, Rfunc, UMIcl, UMIfunc])\n    result_df = pd.DataFrame(results, columns=[\"sample_id\", \"extracted_chain\", \"reads_total\", \"reads_with_umi_pc\", \"reads_aligned_pc\", \"reads_overlapped_aln_pc\",\n                                               \"total_umi\", \"umi_after_correction\", \"overseq_threshold\", \"reads_after_filter\", \"umi_after_filter\",\n                                               \"reads_per_umi\", \"clones_total\", \"reads_in_clones_total\", \"clones\", \"reads_in_clones\", \"clones_func\", \"reads_in_func_clones\", \"umi_in_clones\", \"umi_in_func_clones\"])\n    if not show_offtarget:\n        result_df = result_df.loc[result_df.reads_in_clones/result_df.reads_in_clones_total &gt; off_target_chain_threshold]\n    return result_df.sort_values(by=\"sample_id\").reset_index(drop=True)\n</code></pre>"},{"location":"mixcr/#show_report_images","title":"show_report_images","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"mixcr/#show_report_images_1","title":"show_report_images","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#clonosets","title":"Clonosets","text":"<p>Manipulations with groups of clonosets:</p> <ul> <li>Creates clonosets DataFrames by searching for typical filenames in directories</li> <li>Detects clonoset formats</li> <li>Filters out non-target clonosets (same sample_id but low % of reads)</li> <li>Pools clonosets into one</li> </ul>"},{"location":"modules/#stats","title":"Stats","text":"<ul> <li>Basic clonoset properties, like clone/read/umi counts functional or with OOF/Stops</li> <li>CDR3 amino acid properties: N-counts, physico-chemical properties, Kidera Factors</li> <li>Diversity statistics: observed diversity, (normalized) Shannon-Wiener, chao1</li> <li>Convergence estimate</li> <li>V/D/J/C-gene frequencies or VJ-combinations</li> <li>All calculations are parallelized</li> </ul>"},{"location":"modules/#intersections","title":"Intersections","text":"<ul> <li>Finds parwise intersecting clonotypes between clonosets</li> <li>Intersection metrics: F, F2, D</li> <li>Count tables for clonotypes (similarity groups of clonotypes)</li> <li>Intersect clusters with clonosets</li> <li>TCRnet integration</li> </ul>"},{"location":"modules/#clustering","title":"Clustering","text":"<p>This module implements different immune repertoire clustering analyses:</p> <ul> <li>basic clustering: using hamming distance similarity in CDR3 regions and/or same V/J-segments</li> <li>tcr-dist clustering: using distance metrics from tcrdist software</li> <li>ALICE: find expanded clonotypes by analyzing the probability of neighbour generation with OLGA algrorithm</li> <li>split clusters with community detection algorithms (Louvain, Leiden)</li> <li>easy and modular customisation of cluster analysis</li> <li>output graphs and metadata to Cytoscape format</li> </ul> <p>Graph representation with NetworkX library.</p>"},{"location":"modules/#diffexp","title":"Diffexp","text":"<p>Finds differentially expressing clonotypes/clusters of clonotypes in CFSE-assays or similar experiments.</p>"},{"location":"modules/#clone-filter","title":"Clone Filter","text":"<p>Easy filtering of clonosets by one Filter object, integrated with other analysis procedures. Filtering includes following features:</p> <ul> <li>counting by reads/UMIs/clonotypes</li> <li>use top N clonotypes (tails mixing included for randomly mixing clonotypes with same counts)</li> <li>randomly downsample to N UMIs/reads (you can specify seed, highly recommended for reproducibility)</li> <li>remove low count clonotypes</li> <li>filter out non-functional(OOF,Stop in CDR3)/functional clonotypes</li> <li>white/black list of clonotypes</li> <li>recount frequencies (by reads/UMIs)</li> <li>convert to vdjtools-like format</li> <li>combine (pool) clonotypes with similar features: CDR3/V/J</li> </ul>"},{"location":"modules/#io-module","title":"IO module","text":"<ul> <li>reads and understands clonosets of following formats: MiXCR 3/4, vdjtools, Adaptive Biosciences</li> <li>tsv, .gz, .zip</li> </ul>"},{"location":"modules/#mixcr-module","title":"MiXCR module","text":"<p>As MiXCR is the leading software for generating clonoset tables from raw FastQ files this module helps to run MiXCR 4.3+ batch analyses with SLURM queue manager. Easy accumulation of most sensible processing data from json-reports of MiXCR into one table.</p>"},{"location":"page2/","title":"Page 2","text":""},{"location":"page2/#code-annotation-examples","title":"Code annotation examples","text":""},{"location":"page2/#code-blocks","title":"Code blocks","text":"<p>Some <code>code</code> goes here</p>"},{"location":"page2/#plain-code-blocks","title":"Plain code blocks","text":"<pre><code>def read_json_report(sample_id, folder, report_type):\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    with open(filename) as data_file:    \n        for jsonObj in data_file:\n            report = json.loads(jsonObj)\n    return report\n// some comment\n</code></pre>"},{"location":"page2/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start</p> <pre><code>import sys\nREPSEQ_PATH = '/home/mmyshkin/soft/repseq'\nsys.path.append(REPSEQ_PATH)\nfrom repseq import slurm\nfrom repseq import io\nfrom repseq import common_functions as cf\nfrom repseq import clonosets as cl\nfrom repseq import clustering\nfrom repseq import mixcr as mx\nfrom repseq import segment_usage as su\nfrom repseq import stats\n</code></pre>"},{"location":"page2/#code-with-a-title","title":"Code with a title","text":"import useful packages<pre><code>import os\nimport pandas as pd\nfrom IPython.display import Image, display\nimport json\nimport re\nimport math\nimport random\nimport numpy as np\n</code></pre>"},{"location":"page2/#add-line-numbers","title":"Add line numbers","text":"<pre><code>def shannon_wiener(list_of_numbers):\n    list_of_numbers = list(list_of_numbers)\n    total_size = sum(list_of_numbers)\n    freqs = [s/total_size for s in list_of_numbers]\n    diversity = len(list_of_numbers)\n    sw = -sum([f*np.log(f) for f in freqs])\n    sw_norm = sw/np.log(diversity)\n    return sw, sw_norm, diversity\n</code></pre>"},{"location":"page2/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def shannon_wiener(list_of_numbers):\n    list_of_numbers = list(list_of_numbers)\n    total_size = sum(list_of_numbers)\n    freqs = [s/total_size for s in list_of_numbers]\n    diversity = len(list_of_numbers)\n    sw = -sum([f*np.log(f) for f in freqs])\n    sw_norm = sw/np.log(diversity)\n    return sw, sw_norm, diversity\n</code></pre>"},{"location":"page2/#icons-and-emojis","title":"Icons and Emojis","text":""},{"location":"page2/#try-to-show-all-the-functions-of-a-module","title":"Try to show all the functions of a module","text":""},{"location":"page2/#intersections.find_intersecting_clonotypes","title":"<code>find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>The permissible number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: similar clonotypes by <code>overlap_type</code> in one particular clonoset are NOT combined into one and are treated as different clonotypes.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): The permissible number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset are NOT combined into one\n    and are treated as different clonotypes.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, strict=not bool(mismatches))\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))        \n    else:\n        for i in range(samples_total):\n            for j in range(samples_total):\n                sample1 = sample_list[i]\n                sample2 = sample_list[j]\n                if sample1 != sample2:\n                    tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(find_overlapping_clones_in_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    return pd.concat(result_list).reset_index(drop=True)\n</code></pre>"},{"location":"page2/#intersections.intersect_clones_in_samples_batch","title":"<code>intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type='aaV', by_freq=True, clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating frequencies of intersecting clonotypes between multiple repseq samples. The result of this function may be used for scatterplots of frequencies/counts of  overlapping clonotypes</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>by_umi</code> <code>bool</code> <p>set <code>=True</code> for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>True</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: when using particular overlap type, similar clonotypes in one particular clonoset are combined into one with summation of counts/frequencies.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code> clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type=\"aaV\", by_freq=True, clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating frequencies of intersecting clonotypes between multiple repseq samples.\n    The result of this function may be used for scatterplots of frequencies/counts of \n    overlapping clonotypes\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        by_umi (bool): set `=True` for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: when using particular overlap type, similar clonotypes in one particular clonoset are\n    combined into one with summation of counts/frequencies.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`\n            clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq,\n                                                                                                                        strict=True)\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists))\n    else:\n        for i in range(samples_total):\n            sample1 = sample_list[i]\n            for j in range(samples_total-i-1):\n                sample2 = sample_list[j+i+1]\n                tasks.append((sample1, sample2, clonoset_lists))\n\n    results = run_parallel_calculation(intersect_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    # df = pd.concat(results).index.set_names()\n    df = pd.concat(results).reset_index(drop=True)\n    df = split_tuple_clone_column(df, overlap_type)\n\n    return df\n</code></pre>"},{"location":"page2/#intersections.overlap_distances","title":"<code>overlap_distances(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>The permissible number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required similar clonotypes by <code>overlap_type</code> in one particular clonoset will be combined into one <p>clonotype with sum for count.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def overlap_distances(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): The permissible number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset will be combined into one\n        clonotype with sum for count.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    metric = metric.upper()\n    metrics = [\"F\", \"F2\", \"C\", \"BC\", \"J\", \"JSD\"]\n    mismatch_metrics = [\"F\", \"C\"]\n    non_symmetry_metrics = [\"C\"]\n    frequency_metrics = [\"F\", \"F2\", \"C\"]\n\n\n    if metric not in metrics:\n        raise ValueError(f\"Metric {metric} is not supported. Possible values: {', '.join(metrics)}\")\n\n    if mismatches and metric not in mismatch_metrics:\n        raise ValueError(f\"Metric {metric} does not allow mismatches. Mismatches only possible for: {', '.join(mismatch_metrics)}\")\n\n    by_freq = metric in frequency_metrics\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq)\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))        \n    else:\n        if metric not in non_symmetry_metrics and not two_dataframes:\n            for i in range(samples_total):\n                sample1 = sample_list[i]\n                for j in range(samples_total-i-1):\n                    sample2 = sample_list[j+i+1]\n                    tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n                if metric == \"F2\":\n                    tasks.append((sample1, sample1, clonoset_lists, mismatches, metric))\n        else:\n            for i in range(samples_total):\n                for j in range(samples_total):\n                    sample1 = sample_list[i]\n                    sample2 = sample_list[j]\n                    if sample1 != sample2:\n                        tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(overlap_metric_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    if not two_dataframes and metric != \"C\":\n        result_list = result_list + [(result[1], result[0], result[2]) for result in result_list]\n    overlap_df = pd.DataFrame(result_list, columns=[\"sample1\", \"sample2\", metric.lower()]).pivot_table(index=\"sample1\", columns=[\"sample2\"], values=metric.lower()).reset_index().set_index(\"sample1\").fillna(1)\n    return overlap_df\n</code></pre>"},{"location":"page2/#intersections.prepare_clonotypes_dfs_for_intersections","title":"<code>prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>description</p> required <code>clonosets_df2</code> <code>DataFrame</code> <p>description</p> required <code>cl_filter</code> <code>Filter</code> <p>description</p> required <code>cl_filter2</code> <code>Filter</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>description</p> required <code>by_freq</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>clonoset_lists</code> <code>dict</code> <p>dict of </p> <code>samples_total</code> <code>int</code> <code>two_dataframes</code> <code>bool</code> <code>sample_list</code> <code>list</code> <code>sample_list2</code> <code>list</code> Source code in <code>repseq/intersections.py</code> <pre><code>def prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False):\n    \"\"\"\n    Args:\n        clonosets_df (pd.DataFrame): _description_\n        clonosets_df2 (pd.DataFrame): _description_\n        cl_filter (Filter): _description_\n        cl_filter2 (Filter): _description_\n        overlap_type (str): _description_\n        by_freq (bool, optional): _description_. Defaults to True.\n\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n\n    Returns:\n        clonoset_lists (dict): dict of \n        samples_total (int): \n        two_dataframes (bool):\n        sample_list (list):\n        sample_list2 (list):\n    \"\"\"\n    # output:\n    ### clonoset_lists\n\n    if len(clonosets_df.sample_id.unique()) &lt; len(clonosets_df):\n        raise ValueError(\"Input clonosets in DataFrame have non-unique sample_id's\")\n    clonosets_df_1 = clonosets_df[[\"sample_id\", \"filename\"]]\n    two_dataframes = False\n    if isinstance(clonosets_df2, pd.DataFrame):\n        two_dataframes = True\n        if len(clonosets_df2.sample_id.unique()) &lt; len(clonosets_df2):\n            raise ValueError(\"Input clonosets in DataFrame2 have non-unique sample_id's\")\n        clonosets_df_2 = clonosets_df2[[\"sample_id\", \"filename\"]]\n        intersecting_sample_ids = set(clonosets_df2.sample_id.unique()).intersection(set(clonosets_df.sample_id.unique()))\n        if len(intersecting_sample_ids) &gt; 0 and cl_filter2 is not None:\n            print(\"WARNING! Some samples have the same sample_id in two sample_df's. The second filter will be applied to common samples\")\n\n\n    # converting clonosets to compact lists of clonotypes separated by CDR3 lengths to dictionary based on overlap type and count/freq/umi\n    clonoset_lists = convert_clonosets_to_compact_dicts(clonosets_df_1, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n    if two_dataframes:\n        if cl_filter2 is None:\n            cl_filter2 = cl_filter\n        clonoset_lists_2 = convert_clonosets_to_compact_dicts(clonosets_df_2, cl_filter=cl_filter2,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n        clonoset_lists.update(clonoset_lists_2)\n\n    samples_total = len(clonosets_df_1)\n    if two_dataframes:\n        samples_total = len(pd.concat([clonosets_df_1, clonosets_df_2]))\n\n    sample_list = list(clonosets_df_1.sort_values(by=\"sample_id\").sample_id)\n    sample_list2 = None\n    if two_dataframes:\n        sample_list2 = list(clonosets_df_2.sort_values(by=\"sample_id\").sample_id)\n\n    return clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2\n</code></pre>"},{"location":"supporting_modules/","title":"IO module","text":"<p>This module contains functions for input-output procedures</p>"},{"location":"supporting_modules/#read_yaml_metadata","title":"read_yaml_metadata","text":"<p>Reads NGSiK metadata from a given folder and converts to <code>pd.DataFrame</code>. By default  it searches for <code>metadata.yaml</code> file in this folder and extracts the table.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to NGSiK folder</p> required <code>filename</code> <code>str</code> <p>NGSiK metadata filename</p> <code>'metadata.yaml'</code> <p>Returns:</p> Name Type Description <code>sample_df</code> <code>DataFrame</code> <p>extracted DataFrame from metadata</p> Source code in <code>repseq/io.py</code> <pre><code>def read_yaml_metadata(folder, filename=\"metadata.yaml\"):\n\n    \"\"\"\n    Reads NGSiK metadata from a given folder and converts to `pd.DataFrame`. By default \n    it searches for `metadata.yaml` file in this folder and extracts the table.\n\n    Args:\n        folder (str): path to NGSiK folder\n        filename (str): NGSiK metadata filename\n\n    Returns:\n        sample_df (pd.DataFrame): extracted DataFrame from metadata\n\n    \"\"\"\n\n\n    most_important_columns = [\"sample_id\", \"R1\", \"R2\",\"libraryPerson\", \"projectPerson\", \"projectName\", \"species\", \"miNNNPattern\", \"SMPL\", \"mix_id\", \"preset\", \"startingMaterial\", \"libraryType\"]\n    yaml_filename = os.path.join(folder, filename)\n    with open(yaml_filename, \"r\") as stream:\n        try:\n            metadata_dict =yaml.safe_load(stream)\n    #         pd.io.json.json_normalize(metadata_dict, \"file\", \"samples\", errors='ignore')\n        except yaml.YAMLError as exc:\n            print(exc)\n\n    df = pd.json_normalize(metadata_dict)\n    df = df.explode(\"file\")\n    df = pd.concat([df.drop(['file'], axis=1), df['file'].apply(pd.Series)], axis=1)\n    df = df.explode(\"samples\")\n    df = pd.concat([df.drop(['samples'], axis=1), df['samples'].apply(pd.Series)], axis=1)\n    if 'patternGroupValues' in df.columns:\n        df = pd.concat([df.drop(['patternGroupValues'], axis=1), df['patternGroupValues'].apply(pd.Series)], axis=1)\n    df[\"R1\"] = df[\"R1\"].apply(lambda x: os.path.join(folder, x))\n    df[\"R2\"] = df[\"R2\"].apply(lambda x: os.path.join(folder, x))\n    df = df.rename(columns={\"name\": \"sample_id\"})\n\n    for col_name in most_important_columns[::-1]:\n        if col_name in df.columns:\n            first_column = df.pop(col_name) \n            df.insert(0, col_name, first_column)\n\n    return df.reset_index(drop=True)\n</code></pre>"},{"location":"supporting_modules/#read_clonoset","title":"read_clonoset","text":"<p>Reads generic clonoset files.  Easyly reads <code>csv</code>, <code>tsv</code>, <code>txt</code> or <code>gz</code> files. Reads first found file inside <code>zip</code> files. Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to clonoset file</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>DataFrame representation of clonoset in given file. Bioadaptive clonosets are converted to vdjtools-like format.</p> Source code in <code>repseq/io.py</code> <pre><code>def read_clonoset(filename):\n    \"\"\"\n    Reads generic clonoset files. \n    Easyly reads `csv`, `tsv`, `txt` or `gz` files.\n    Reads first found file inside `zip` files.\n    Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive\n\n    Args:\n        filename (str): path to clonoset file\n\n    Returns:\n        clonoset (pd.DataFrame): DataFrame representation of clonoset in given file.\n            Bioadaptive clonosets are converted to vdjtools-like format.\n    \"\"\"\n\n\n    file_name, file_extension = os.path.splitext(filename)\n\n    d_types_mixcr = {'cloneId': int, 'readCount': int, 'readFraction': float,\n                    'uniqueUMICount': int, 'uniqueUMIFraction': float,\n                    'uniqueMoleculeCount': int, 'uniqueMoleculeFraction': float,\n                    'cloneCount': int, 'cloneFraction': float,\n                    'targetSequences': str, 'targetQualities': str,\n                    'allVHitsWithScore': str, 'allDHitsWithScore': str,\n                    'allJHitsWithScore': str, 'allCHitsWithScore': str,\n                    'allVAlignments': str, 'allDAlignments': str,\n                    'allJAlignments': str, 'allCAlignments': str,\n                    'nSeqFR1': str, 'minQualFR1': str,\n                    'nSeqCDR1': str, 'minQualCDR1': str,\n                    'nSeqFR2': str, 'minQualFR2': str,\n                    'nSeqCDR2': str, 'minQualCDR2': str,\n                    'nSeqFR3': str, 'minQualFR3': str,\n                    'nSeqCDR3': str, 'minQualCDR3': str,\n                    'nSeqFR4': str, 'minQualFR4': str,\n                    'aaSeqFR1': str, 'aaSeqCDR1': str,\n                    'aaSeqFR2': str, 'aaSeqCDR2': str,\n                    'aaSeqFR3': str, 'aaSeqCDR3': str,\n                    'aaSeqFR4': str, 'refPoints': str\n                    }\n\n    d_types_vdjtools = {'cdr3aa': str, 'cdr3nt': str,\n                        'v': str, 'd': str, 'j': str,\n                        'CDR3aa': str, 'CDR3nt': str,\n                        'V': str, 'D': str, 'J': str,\n                        'C': str, \"frequency\": float#,\n                        #'count': int, 'freq': float#,\n                        #'VEnd':int, 'DStart':int, 'DEnd':int, \"JStart\":int\n                        }\n\n    d_types_bioadaptive = {'nucleotide': str, 'aminoAcid': str,\n                            'count (templates/reads)': int,\n                            'frequencyCount (%)': float,\n                            'vGeneName': str, 'dGeneName': str,\n                            'jGeneName': str, 'cdr3Length': int,\n                            'n1Index': int,'dIndex': int,\n                            'n2Index': int,'jIndex': int\n                            }\n\n\n    datatypes = {**d_types_mixcr,**d_types_vdjtools, **d_types_bioadaptive}\n    if file_extension == \".zip\":\n        archive = zipfile.ZipFile(filename, 'r')\n        inner_filename = zipfile.ZipFile.namelist(archive)[0]\n        filename = archive.open(inner_filename)\n    clonoset = pd.read_csv(filename, sep=\"\\t\", dtype=datatypes)\n    if 'count (templates/reads)' in clonoset.columns:\n        clonoset = convert_bioadaptive_clonoset(clonoset)\n    return clonoset\n</code></pre>"},{"location":"supporting_modules/#read_json_report","title":"read_json_report","text":"<p>Reads MiXCR4 json reports into a Python mixed data structure. This function takes the last json record, if for example MiXCR adds up several records  to json file (it happens, when the program is rerun several times on the same data). Program also includes cases when Sample-barcodes are used.</p> <p>Parameters:</p> Name Type Description Default <code>sample_id</code> <code>str</code> <p>sample_id used when running the MiXCR program</p> required <code>folder</code> <code>str</code> <p>folder in which the MiXCR output is stored</p> required <code>report_type</code> <code>str</code> <p>align, refine, assemble</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>mixed dict/list python structure, representing the json report</p> Source code in <code>repseq/io.py</code> <pre><code>def read_json_report(sample_id, folder, report_type):\n    \"\"\"\n    Reads MiXCR4 json reports into a Python mixed data structure.\n    This function takes the last json record, if for example MiXCR adds up several records \n    to json file (it happens, when the program is rerun several times on the same data).\n    Program also includes cases when Sample-barcodes are used.\n\n    Args:\n        sample_id (str): sample_id used when running the MiXCR program\n        folder (str): folder in which the MiXCR output is stored\n        report_type (str): align, refine, assemble\n\n    Returns:\n        report (dict): mixed dict/list python structure, representing the json report\n    \"\"\"\n\n\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    if \".\" in sample_id:\n        sample_id2 = \".\".join(sample_id.split(\".\")[:-1])\n        filename2 = os.path.join(folder, f\"{sample_id2}.{report_type}.report.json\")\n        try:\n            report = open_json_report(filename)\n        except FileNotFoundError:\n            report = open_json_report(filename2)\n    else:\n        report = open_json_report(filename)\n    return report\n</code></pre>"},{"location":"usage_clustering/","title":"Usage: clustering","text":""},{"location":"usage_diffexp/","title":"Usage: differential expression analysis","text":""},{"location":"usage_intersections/","title":"Usage: intersections between clonosets","text":""},{"location":"usage_mixcr/","title":"Usage: working with MiXCR","text":"<p>MiXCR is the leading software for generating clonoset tables from raw FastQ files. MiXCR module allows to run MiXCR 4.3+ batch analyses with SLURM queue manager.</p> <p>Environment</p> <p>Before getting started, make sure that main_repseq environment is chosen</p> <pre><code># 1) Create sample_df from dataset metadata in .yaml format (if it's in a tabular format, use external libraries such as Pandas). \n# Remove unnesessary columns if needed.\n# If METADATA_FILENAME is absent, it is set to `metadata.yaml` by default. If your dataset does not have metadata, create the dataframe manually. \n# The neccessary columns are: R1, R2, sample_id, where R1 and R2 contain paths (using full paths is advised) to respective raw files, \n# and sample_id are arbitrary unique identificators\nsample_df = repseqio.repseqio.read_yaml_metadata(RAW_DATA_DIR, METADATA_FILENAME)\n\n# 2) Create a command template for mixcr analyze. The default template is `mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix`. \n# In case of Human 7GENES DNA Multiplex MiXCR built-in preset, no template is needed.\nmixcr_race_command_template = \"mixcr analyze milab-human-rna-tcr-umi-race -f r1 r2 output_prefix\"\n\n# 3) Run mixcr analyze in batches.\nmx.mixcr4_analyze_batch(sample_df, MIXCR_DIR, command_template=mixcr_race_command_template,\n                        mixcr_path=MIXCR, memory=32, time_estimate=1.5)\n</code></pre>"},{"location":"usage_stats/","title":"Usage: calculating basic stats","text":""}]}